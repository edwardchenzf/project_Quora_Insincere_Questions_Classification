{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4714739f681baf00e14aae4340911bfb064d502"
   },
   "source": [
    "# Summary\n",
    "This kernel is a cleaned version of my submission scripts. I learn a lot from this challenge. In short, fast geometric ensembling gives me an incredible  boost in performance and bucket iterator makes it possible in 7200s. Minor improvements are made based on great kernels. Thank you all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "60073a35d59b06c35dd8607270973de0c39f14a1"
   },
   "source": [
    "# References\n",
    "model structure & clr from https://www.kaggle.com/shujian/single-rnn-with-4-folds-clr  \n",
    "hidden size 256 from https://www.kaggle.com/artgor/text-modelling-in-pytorch  \n",
    "speed up pre-processing from https://www.kaggle.com/syhens/speed-up-your-preprocessing  \n",
    "the idea to reduce oov from https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings  \n",
    "misspell dictionary & punctuations from https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing  \n",
    "latex cleaning from https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage  \n",
    "pytorch text processing routines from https://github.com/howardyclo/pytorch-seq2seq-example/blob/master/seq2seq.ipynb  \n",
    "capsule  from https://www.kaggle.com/spirosrap/bilstm-attention-kfold-clr-extra-features-capsule  \n",
    "Please correct me if I miss any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "225fd605f3437e873588c6c4af826ba208722390"
   },
   "source": [
    "# Some new things\n",
    "## performance\n",
    "**fast geometric ensemble** from https://arxiv.org/abs/1802.10026. It gives a consistant and significant boost in both LB score and CV score for various models when combined with a learnable embedding.  \n",
    "**semi-supervised ensemble** similar to [Malware Classification Challenge 1st solution]( https://www.kaggle.com/c/malware-classification/discussion/13897).    ~~Marginal significance can be observed with a large test set.~~   it doesn't bring me any benefits in the 2nd stage.  \n",
    "**\"mix up\" embeddings**. The idea is to randomly choose a linear combination between two embeddings rather  than simple averaging. Though no significant improvement can be observed, I still keep it in my solution as  regularization.\n",
    "## speed\n",
    "**bucket iterator**. similar to the one in torchtext.  It  runs twice as fast as static padding.  \n",
    "## miscs   \n",
    "  - speed up  capsule.  \n",
    "  - load embedding file with pandas. It saves ~80 seconds per embedding.  \n",
    "  - reduce oov by  replacing oov word with its capitized, upper, lower version if  available. The final oov rate is about 7.5%. \n",
    "  - minor changes to the model structure. I don't think they really work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "93ba7ed9b78ef617b3597a2ccac445bc6aaf8cd3"
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "embedding_glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "embedding_fasttext = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "embedding_para = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "embedding_w2v = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "train_path = '../input/train.csv'\n",
    "test_path = '../input/test.csv'\n",
    "\n",
    "mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "                \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\",\n",
    "                \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"I will have\", \"i'm\": \"i am\",\n",
    "                \"i've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "                \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "                \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                \"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n",
    "                \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
    "                \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "                \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "                \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
    "                \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "                \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
    "                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "                \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color',\n",
    "                'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n",
    "                'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n",
    "                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize',\n",
    "                'youtu ': 'youtube ', 'qoura': 'quora', 'sallary': 'salary', 'whta': 'what',\n",
    "                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
    "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doi': 'do I',\n",
    "                'thebest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation',\n",
    "                'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis',\n",
    "                'etherium': 'ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017',\n",
    "                '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess',\n",
    "                \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "                'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "puncts = '\\'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\",\n",
    "                 \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '”': '\"', '“': '\"', \"£\": \"e\",\n",
    "                 '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta',\n",
    "                 '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "for p in puncts:\n",
    "    punct_mapping[p] = ' %s ' % p\n",
    "\n",
    "p = re.compile('(\\[ math \\]).+(\\[ / math \\])')\n",
    "p_space = re.compile(r'[^\\x20-\\x7e]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "80ad5cd50c433f08102f77daf07bb0594a513cd6"
   },
   "outputs": [],
   "source": [
    "#  seeding functions\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed + 1)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed + 2)\n",
    "    random.seed(seed + 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "e159b30ba5b97c14c69076f9fc4fde5dd7559c61"
   },
   "outputs": [],
   "source": [
    "#data loading & pre-processing\n",
    "\n",
    "def clean_text(text):\n",
    "    # clean latex maths\n",
    "    text = p.sub(' [ math ] ', text)\n",
    "    # clean invisible chars\n",
    "    text = p_space.sub(r'', text)\n",
    "    # clean punctuations\n",
    "    for punct in punct_mapping:\n",
    "        if punct in text:\n",
    "            text = text.replace(punct, punct_mapping[punct])\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        # replace contractions & correct misspells\n",
    "        token = mispell_dict.get(token.lower(), token)\n",
    "        tokens.append(token)\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "def load_data(train_path=train_path, test_path=test_path, debug=False):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    if debug:\n",
    "        train_df = train_df[:10000]\n",
    "        test_df = test_df[:10000]\n",
    "    s = time.time()\n",
    "    train_df['question_text'] = train_df['question_text'].apply(clean_text)\n",
    "    test_df['question_text'] = test_df['question_text'].apply(clean_text)\n",
    "    print('preprocssing {}s'.format(time.time() - s))\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "6054064f89de5eaffb3a697ab81b241af2af13af"
   },
   "outputs": [],
   "source": [
    "# vocabulary functions\n",
    "def build_counter(sents, splited=False):\n",
    "    counter = Counter()\n",
    "    for sent in tqdm(sents, ascii=True, desc='building conuter'):\n",
    "        if splited:\n",
    "            counter.update(sent)\n",
    "        else:\n",
    "            counter.update(sent.split())\n",
    "    return counter\n",
    "\n",
    "\n",
    "def build_vocab(counter, max_vocab_size):\n",
    "    vocab = {'token2id': {'<PAD>': 0, '<UNK>': max_vocab_size + 1}}\n",
    "    vocab['token2id'].update(\n",
    "        {token: _id + 1 for _id, (token, count) in\n",
    "         tqdm(enumerate(counter.most_common(max_vocab_size)), desc='building vocab')})\n",
    "    vocab['id2token'] = {v: k for k, v in vocab['token2id'].items()}\n",
    "    return vocab\n",
    "\n",
    "def tokens2ids(tokens, token2id):\n",
    "    seq = []\n",
    "    for token in tokens:\n",
    "        token_id = token2id.get(token, len(token2id) - 1)\n",
    "        seq.append(token_id)\n",
    "    return seq\n",
    "\n",
    "#  data set\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, vocab=None, num_max=None, max_seq_len=100,\n",
    "                 max_vocab_size=95000):\n",
    "        if num_max is not None:\n",
    "            df = df[:num_max]\n",
    "\n",
    "        self.src_sents = df['question_text'].tolist()\n",
    "        self.qids = df['qid'].values\n",
    "        if vocab is None:\n",
    "            src_counter = build_counter(self.src_sents)\n",
    "            vocab = build_vocab(src_counter, max_vocab_size)\n",
    "        self.vocab = vocab\n",
    "        if 'src_seqs' not in df.columns:\n",
    "            self.src_seqs = []\n",
    "            for sent in tqdm(self.src_sents, desc='tokenize'):\n",
    "                seq = tokens2ids(sent.split()[:max_seq_len], vocab['token2id'])\n",
    "                self.src_seqs.append(seq)\n",
    "        else:\n",
    "            self.src_seqs = df['src_seqs'].tolist()\n",
    "        if 'target' in df.columns:\n",
    "            self.targets = df['target'].values\n",
    "        else:\n",
    "            self.targets = np.random.randint(2, size=(len(self.src_sents),))\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sents)\n",
    "\n",
    "    # for bucket iterator\n",
    "    def get_keys(self):\n",
    "        lens = np.fromiter(\n",
    "            tqdm(((min(self.max_seq_len, len(c.split()))) for c in self.src_sents), desc='generate lens'),\n",
    "            dtype=np.int32)\n",
    "        return lens\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.qids[index], self.src_sents[index], self.src_seqs[index], self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "5d0a7804f6f9bc3973495b1ca8da0d0f0de39316"
   },
   "outputs": [],
   "source": [
    "\n",
    "#  dynamic padding\n",
    "def _pad_sequences(seqs):\n",
    "    lens = [len(seq) for seq in seqs]\n",
    "    max_len = max(lens)\n",
    "\n",
    "    padded_seqs = torch.zeros(len(seqs), max_len).long()\n",
    "    for i, seq in enumerate(seqs):\n",
    "        end = lens[i]\n",
    "        padded_seqs[i, :end] = torch.LongTensor(seq)\n",
    "    return padded_seqs, lens\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    qids, src_sents, src_seqs, targets, = zip(*data)\n",
    "    src_seqs, src_lens = _pad_sequences(src_seqs)\n",
    "    return qids, src_sents, src_seqs, src_lens, torch.FloatTensor(targets)\n",
    "\n",
    "\n",
    "#  bucket iterator\n",
    "def divide_chunks(l, n):\n",
    "    if n == len(l):\n",
    "        yield np.arange(len(l), dtype=np.int32), l\n",
    "    else:\n",
    "        # looping till length l\n",
    "        for i in range(0, len(l), n):\n",
    "            data = l[i:i + n]\n",
    "            yield np.arange(i, i + len(data), dtype=np.int32), data\n",
    "\n",
    "\n",
    "def prepare_buckets(lens, bucket_size, batch_size, shuffle_data=True, indices=None):\n",
    "    lens = -lens\n",
    "    assert bucket_size % batch_size == 0 or bucket_size == len(lens)\n",
    "    if indices is None:\n",
    "        if shuffle_data:\n",
    "            indices = shuffle(np.arange(len(lens), dtype=np.int32))\n",
    "            lens = lens[indices]\n",
    "        else:\n",
    "            indices = np.arange(len(lens), dtype=np.int32)\n",
    "    new_indices = []\n",
    "    extra_batch = None\n",
    "    for chunk_index, chunk in (divide_chunks(lens, bucket_size)):\n",
    "        # sort indices in bucket by descending order of length\n",
    "        indices_sorted = chunk_index[np.argsort(chunk, axis=-1)]\n",
    "        batches = []\n",
    "        for _, batch in divide_chunks(indices_sorted, batch_size):\n",
    "            if len(batch) == batch_size:\n",
    "                batches.append(batch.tolist())\n",
    "            else:\n",
    "                assert extra_batch is None\n",
    "                assert batch is not None\n",
    "                extra_batch = batch\n",
    "        # shuffling batches within buckets\n",
    "        if shuffle_data:\n",
    "            batches = shuffle(batches)\n",
    "        for batch in batches:\n",
    "            new_indices.extend(batch)\n",
    "\n",
    "    if extra_batch is not None:\n",
    "        new_indices.extend(extra_batch)\n",
    "    return indices[new_indices]\n",
    "\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, sort_keys, bucket_size=None, batch_size=1536, shuffle_data=True):\n",
    "        super().__init__(data_source)\n",
    "        self.shuffle = shuffle_data\n",
    "        self.batch_size = batch_size\n",
    "        self.sort_keys = sort_keys\n",
    "        self.bucket_size = bucket_size if bucket_size is not None else len(sort_keys)\n",
    "        if not shuffle_data:\n",
    "            self.index = prepare_buckets(self.sort_keys, bucket_size=self.bucket_size, batch_size=self.batch_size,\n",
    "                                         shuffle_data=self.shuffle)\n",
    "        else:\n",
    "            self.index = None\n",
    "        self.weights = None\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        assert w >= 0\n",
    "        total = np.sum(w)\n",
    "        if total != 1:\n",
    "            w = w / total\n",
    "        self.weights = w\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = None\n",
    "        if self.weights is not None:\n",
    "            total = len(self.sort_keys)\n",
    "\n",
    "            indices = np.random.choice(total, (total,), p=self.weights)\n",
    "        if self.shuffle:\n",
    "            self.index = prepare_buckets(self.sort_keys, bucket_size=self.bucket_size, batch_size=self.batch_size,\n",
    "                                         shuffle_data=self.shuffle, indices=indices)\n",
    "        return iter(self.index)\n",
    "\n",
    "    def get_reverse_indexes(self):\n",
    "        indexes = np.zeros((len(self.index),), dtype=np.int32)\n",
    "        for i, j in enumerate(self.index):\n",
    "            indexes[j] = i\n",
    "        return indexes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sort_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "9f3a6717a830188b4603e31a0666c25b800aff34"
   },
   "outputs": [],
   "source": [
    "# embedding stuffs\n",
    "def read_embedding(embedding_file):\n",
    "    \"\"\"\n",
    "    read embedding file into a dictionary\n",
    "    each line of the embedding file should in the format like  word 0.13 0.22 ... 0.44\n",
    "    :param embedding_file: path of the embedding.\n",
    "    :return: a dictionary of word to its embedding (numpy array)\n",
    "    \"\"\"\n",
    "    if os.path.basename(embedding_file) != 'wiki-news-300d-1M.vec':\n",
    "        skip_head = None\n",
    "    else:\n",
    "        skip_head = 0\n",
    "    if os.path.basename(embedding_file) == 'paragram_300_sl999.txt':\n",
    "        encoding = 'latin'\n",
    "    else:\n",
    "        encoding = 'utf-8'\n",
    "    embeddings_index = {}\n",
    "    t_chunks = pd.read_csv(embedding_file, index_col=0, skiprows=skip_head, encoding=encoding, sep=' ', header=None,\n",
    "                           quoting=3,\n",
    "                           doublequote=False, quotechar=None, engine='c', na_filter=False, low_memory=True,\n",
    "                           chunksize=10000)\n",
    "    for t in t_chunks:\n",
    "        for k, v in zip(t.index.values, t.values):\n",
    "            embeddings_index[k] = v.astype(np.float32)\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_emb(embedding_index, word, word_raw):\n",
    "    if word == word_raw:\n",
    "        return None\n",
    "    else:\n",
    "        return embedding_index.get(word, None)\n",
    "\n",
    "\n",
    "def embedding2numpy(embedding_path, word_index, num_words, embed_size, emb_mean=0., emb_std=0.5,\n",
    "                    report_stats=False):\n",
    "    embedding_index = read_embedding(embedding_path)\n",
    "    num_words = min(num_words + 2, len(word_index))\n",
    "    if report_stats:\n",
    "        all_coefs = []\n",
    "        for v in embedding_index.values():\n",
    "            all_coefs.append(v.reshape([-1, 1]))\n",
    "        all_coefs = np.concatenate(all_coefs)\n",
    "        print(all_coefs.mean(), all_coefs.std(), np.linalg.norm(all_coefs, axis=-1).mean())\n",
    "    embedding_matrix = np.zeros((num_words, embed_size), dtype=np.float32)\n",
    "    oov = 0\n",
    "    oov_cap = 0\n",
    "    oov_upper = 0\n",
    "    oov_lower = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i == 0:  # padding\n",
    "            continue\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word, None)\n",
    "        if embedding_vector is None:\n",
    "            embedding_vector = get_emb(embedding_index, word.lower(), word)\n",
    "            if embedding_vector is None:\n",
    "                embedding_vector = get_emb(embedding_index, word.upper(), word)\n",
    "                if embedding_vector is None:\n",
    "                    embedding_vector = get_emb(embedding_index, word.capitalize(), word)\n",
    "                    if embedding_vector is None:\n",
    "                        oov += 1\n",
    "                        # embedding_vector = (np.zeros((1, embed_size)))\n",
    "                        embedding_vector = np.random.normal(emb_mean, emb_std, size=(1, embed_size))\n",
    "                    else:\n",
    "                        oov_lower += 1\n",
    "                else:\n",
    "                    oov_upper += 1\n",
    "            else:\n",
    "                oov_cap += 1\n",
    "\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    print('oov %d/%d/%d/%d/%d' % (oov, oov_cap, oov_upper, oov_lower, len(word_index)))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def load_embedding(vocab, max_vocab_size, embed_size):\n",
    "    # load embedding\n",
    "    embedding_matrix1 = embedding2numpy(embedding_glove, vocab['token2id'], max_vocab_size, embed_size,\n",
    "                                        emb_mean=-0.005838499, emb_std=0.48782197, report_stats=False)\n",
    "    # -0.005838499 0.48782197 0.37823704\n",
    "    # oov 9196\n",
    "    # embedding_matrix2 = embedding2numpy(embedding_fasttext, vocab.token2id, max_vocab_size, embed_size,\n",
    "    #                                    report_stats=False, emb_mean=-0.0033469985, emb_std=0.109855495, )\n",
    "    # -0.0033469985 0.109855495 0.07475414\n",
    "    # oov 12885\n",
    "    embedding_matrix2 = embedding2numpy(embedding_para, vocab['token2id'], max_vocab_size, embed_size,\n",
    "                                        emb_mean=-0.0053247833, emb_std=0.49346462, report_stats=False)\n",
    "    # -0.0053247833 0.49346462 0.3828983\n",
    "    # oov 9061\n",
    "    # embedding_w2v\n",
    "    # -0.003527845 0.13315111 0.09407869\n",
    "    # oov 18927\n",
    "    return [embedding_matrix1, embedding_matrix2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d51ddd85f8f4f58245bd65d5bf74cf3bd36ce9e6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# cyclic learning rate\n",
    "def set_lr(optimizer, lr):\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "\n",
    "class CyclicLR:\n",
    "    def __init__(self, optimizer, base_lr=0.001, max_lr=0.002, step_size=300., mode='triangular',\n",
    "                 gamma=0.99994, scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "        self.optimizer = optimizer\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** x\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        if new_base_lr is not None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr is not None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size is not None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n",
    "                self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self):\n",
    "        if self.clr_iterations == 0:\n",
    "            set_lr(self.optimizer, self.base_lr)\n",
    "        else:\n",
    "            set_lr(self.optimizer, self.clr())\n",
    "\n",
    "    def on_batch_end(self):\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        set_lr(self.optimizer, self.clr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "4662e44441a1e128fef47cc7a0860f4b7aeaf398"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class Capsule(nn.Module):\n",
    "    def __init__(self, input_dim_capsule=1024, num_capsule=5, dim_capsule=5, routings=4):\n",
    "        super(Capsule, self).__init__()\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.activation = self.squash\n",
    "        self.W = nn.Parameter(\n",
    "            nn.init.xavier_normal_(torch.empty(1, input_dim_capsule, self.num_capsule * self.dim_capsule)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        u_hat_vecs = torch.matmul(x, self.W)\n",
    "        batch_size = x.size(0)\n",
    "        input_num_capsule = x.size(1)\n",
    "        u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
    "                                      self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = u_hat_vecs.permute(0, 2, 1,\n",
    "                                        3).contiguous()  # (batch_size,num_capsule,input_num_capsule,dim_capsule)\n",
    "        with torch.no_grad():\n",
    "            b = torch.zeros_like(u_hat_vecs[:, :, :, 0])\n",
    "        for i in range(self.routings):\n",
    "            c = torch.nn.functional.softmax(b, dim=1)  # (batch_size,num_capsule,input_num_capsule)\n",
    "            outputs = self.activation(torch.sum(c.unsqueeze(-1) * u_hat_vecs, dim=2))  # bij,bijk->bik\n",
    "            if i < self.routings - 1:\n",
    "                b = (torch.sum(outputs.unsqueeze(2) * u_hat_vecs, dim=-1))  # bik,bijk->bij\n",
    "        return outputs  # (batch_size, num_capsule, dim_capsule)\n",
    "\n",
    "    def squash(self, x, axis=-1):\n",
    "        s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
    "        scale = torch.sqrt(s_squared_norm + 1e-7)\n",
    "        return x / scale\n",
    "\n",
    "\n",
    "#  model\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, max_seq_len=70):\n",
    "        super().__init__()\n",
    "        self.attention_fc = nn.Linear(feature_dim, 1)\n",
    "        self.bias = nn.Parameter(torch.zeros(1, max_seq_len, 1, requires_grad=True))\n",
    "\n",
    "    def forward(self, rnn_output):\n",
    "        \"\"\"\n",
    "        forward attention scores and attended vectors\n",
    "        :param rnn_output: (#batch,#seq_len,#feature)\n",
    "        :return: attended_outputs (#batch,#feature)\n",
    "        \"\"\"\n",
    "        attention_weights = self.attention_fc(rnn_output)\n",
    "        seq_len = rnn_output.size(1)\n",
    "        attention_weights = self.bias[:, :seq_len, :] + attention_weights\n",
    "        attention_weights = torch.tanh(attention_weights)\n",
    "        attention_weights = torch.exp(attention_weights)\n",
    "        attention_weights_sum = torch.sum(attention_weights, dim=1, keepdim=True) + 1e-7\n",
    "        attention_weights = attention_weights / attention_weights_sum\n",
    "        attended = torch.sum(attention_weights * rnn_output, dim=1)\n",
    "        return attended\n",
    "\n",
    "\n",
    "class InsincereModel(nn.Module):\n",
    "    def __init__(self, device, hidden_dim, hidden_dim_fc, embedding_matrixs, vocab_size=None, embedding_dim=None,\n",
    "                 dropout=0.1, num_capsule=5, dim_capsule=5, capsule_out_dim=1, alpha=0.8, beta=0.8,\n",
    "                 finetuning_vocab_size=120002,\n",
    "                 embedding_mode='mixup', max_seq_len=70):\n",
    "        super(InsincereModel, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.embedding_mode = embedding_mode\n",
    "        self.finetuning_vocab_size = finetuning_vocab_size\n",
    "        self.alpha = alpha\n",
    "        vocab_size, embedding_dim = embedding_matrixs[0].shape\n",
    "        self.raw_embedding_weights = embedding_matrixs\n",
    "        self.embedding_0 = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).from_pretrained(\n",
    "            torch.from_numpy(embedding_matrixs[0]))\n",
    "        self.embedding_1 = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).from_pretrained(\n",
    "            torch.from_numpy(embedding_matrixs[1]))\n",
    "        self.embedding_mean = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).from_pretrained(\n",
    "            torch.from_numpy((embedding_matrixs[0] + embedding_matrixs[1]) / 2))\n",
    "        self.learnable_embedding = nn.Embedding(finetuning_vocab_size, embedding_dim, padding_idx=0)\n",
    "        nn.init.constant_(self.learnable_embedding.weight, 0)\n",
    "        self.learn_embedding = False\n",
    "        self.spatial_dropout = nn.Dropout2d(p=0.2)\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn0 = nn.LSTM(embedding_dim, int(hidden_dim / 2), num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.rnn1 = nn.GRU(hidden_dim, int(hidden_dim / 2), num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.capsule = Capsule(input_dim_capsule=self.hidden_dim, num_capsule=num_capsule, dim_capsule=dim_capsule)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.lincaps = nn.Linear(num_capsule * dim_capsule, capsule_out_dim)\n",
    "        self.attention1 = Attention(self.hidden_dim, max_seq_len=max_seq_len)\n",
    "        self.attention2 = Attention(self.hidden_dim, max_seq_len=max_seq_len)\n",
    "        self.fc = nn.Linear(hidden_dim * 4 + capsule_out_dim, hidden_dim_fc)\n",
    "        self.norm = torch.nn.LayerNorm(hidden_dim * 4 + capsule_out_dim)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout_linear = nn.Dropout(p=dropout)\n",
    "        self.hidden2out = nn.Linear(hidden_dim_fc, 1)\n",
    "\n",
    "    def set_embedding_mode(self, embedding_mode):\n",
    "        self.embedding_mode = embedding_mode\n",
    "\n",
    "    def enable_learning_embedding(self):\n",
    "        self.learn_embedding = True\n",
    "\n",
    "    def init_weights(self):\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for k in ih:\n",
    "            nn.init.xavier_uniform_(k)\n",
    "        for k in hh:\n",
    "            nn.init.orthogonal_(k)\n",
    "        for k in b:\n",
    "            nn.init.constant_(k, 0)\n",
    "\n",
    "    def apply_spatial_dropout(self, emb):\n",
    "        emb = emb.permute(0, 2, 1).unsqueeze(-1)\n",
    "        emb = self.spatial_dropout(emb).squeeze(-1).permute(0, 2, 1)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, seqs, lens, return_logits=True):\n",
    "        # forward embeddings\n",
    "        if self.embedding_mode == 'mixup':\n",
    "            emb0 = self.embedding_0(seqs)  # batch_size x seq_len x embedding_dim\n",
    "            emb1 = self.embedding_1(seqs)\n",
    "            prob = np.random.beta(self.alpha, self.beta, size=(seqs.size(0), 1, 1)).astype(np.float32)\n",
    "            prob = torch.from_numpy(prob).to(self.device)\n",
    "            emb = emb0 * prob + emb1 * (1 - prob)\n",
    "        elif self.embedding_mode == 'emb0':\n",
    "            emb = self.embedding_0(seqs)\n",
    "        elif self.embedding_mode == 'emb1':\n",
    "            emb = self.embedding_1(seqs)\n",
    "        elif self.embedding_mode == 'mean':\n",
    "            emb = self.embedding_mean(seqs)\n",
    "        else:\n",
    "            assert False\n",
    "        if self.learn_embedding:\n",
    "            seq_clamped = torch.clamp(seqs, 0, self.finetuning_vocab_size - 1)\n",
    "            emb_learned = self.learnable_embedding(seq_clamped)\n",
    "            emb = emb + emb_learned\n",
    "        emb = self.apply_spatial_dropout(emb)\n",
    "        # forward rnn encoder\n",
    "        lstm_output0, _ = self.rnn0(emb)\n",
    "        lstm_output1, _ = self.rnn1(lstm_output0)\n",
    "        # forward capsule\n",
    "        content3 = self.capsule(lstm_output1)\n",
    "        batch_size = content3.size(0)\n",
    "        content3 = content3.view(batch_size, -1)\n",
    "        content3 = self.dropout2(content3)\n",
    "        content3 = torch.relu(self.lincaps(content3))\n",
    "        # forward feature extractor\n",
    "        feature_att1 = self.attention1(lstm_output0)\n",
    "        feature_att2 = self.attention2(lstm_output1)\n",
    "        feature_avg2 = torch.mean(lstm_output1, dim=1)\n",
    "        feature_max2, _ = torch.max(lstm_output1, dim=1)\n",
    "        feature = torch.cat((feature_att1, feature_att2, feature_avg2, feature_max2, content3), dim=-1)\n",
    "        feature = self.norm(feature)\n",
    "        feature = self.dropout1(feature)\n",
    "        feature = torch.relu(feature)\n",
    "        # forward dense layer\n",
    "        out = self.fc(feature)\n",
    "        out = self.dropout_linear(out)\n",
    "        out = self.hidden2out(out)  # batch_size x 1\n",
    "        if not return_logits:\n",
    "            out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "3b4a7fc4e688e1e3b7d50a8449b13803edb7734c"
   },
   "outputs": [],
   "source": [
    "\n",
    "#  util functions\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def margin_score(targets, predictions):\n",
    "    return ((targets == 1) * (1 - predictions) + (targets == 0) * (predictions)).mean()\n",
    "\n",
    "\n",
    "def report_perf(valid_dataset, predictions_va, threshold, idx, epoch_cur, desc='val set'):\n",
    "    val_f1 = f1_score(valid_dataset.targets, predictions_va > threshold)\n",
    "    val_auc = roc_auc_score(valid_dataset.targets, predictions_va)\n",
    "    val_margin = margin_score(valid_dataset.targets, predictions_va)\n",
    "    print('idx {} epoch {} {} f1 : {:.4f} auc : {:.4f} margin : {:.4f}'.format(\n",
    "        idx,\n",
    "        epoch_cur,\n",
    "        desc,\n",
    "        val_f1,\n",
    "        val_auc,\n",
    "        val_margin))\n",
    "\n",
    "\n",
    "def get_gpu_memory_usage(device_id):\n",
    "    return round(torch.cuda.max_memory_allocated(device_id) / 1000 / 1000)\n",
    "\n",
    "\n",
    "def avg(loss_list):\n",
    "    if len(loss_list) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sum(loss_list) / len(loss_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "1fa965a4f849f08ac729353ecd3bdb7ecdeafb9d"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# evaluation\n",
    "def eval_model(model, data_iter, device, order_index=None):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in data_iter:\n",
    "            qid_batch, src_sents, src_seqs, src_lens, tgts = batch_data\n",
    "            src_seqs = src_seqs.to(device)\n",
    "            out = model(src_seqs, src_lens, return_logits=False)\n",
    "            predictions.append(out)\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    if order_index is not None:\n",
    "        predictions = predictions[order_index]\n",
    "    predictions = predictions.to('cpu').numpy().ravel()\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "1ed9297b3b185bfb6b8bf04a819e9cc5d3cee100"
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "def cv(train_df, test_df, device=None, n_folds=10, shared_resources=None, share=True, **kwargs):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:{}\".format(0) if torch.cuda.is_available() else \"cpu\")\n",
    "    max_vocab_size = kwargs['max_vocab_size']\n",
    "    embed_size = kwargs['embed_size']\n",
    "    threshold = kwargs['threshold']\n",
    "    max_seq_len = kwargs['max_seq_len']\n",
    "    if shared_resources is None:\n",
    "        shared_resources = {}\n",
    "    if share:\n",
    "        if 'vocab' not in shared_resources:\n",
    "            # also include the test set\n",
    "\n",
    "            counter = build_counter(chain(train_df['question_text'], test_df['question_text']))\n",
    "            vocab = build_vocab(counter, max_vocab_size=max_vocab_size)\n",
    "            shared_resources['vocab'] = vocab\n",
    "            # tokenize sentences\n",
    "            seqs = []\n",
    "            for sent in tqdm(train_df['question_text'], desc='tokenize'):\n",
    "                seq = tokens2ids(sent.split()[:max_seq_len], vocab['token2id'])\n",
    "                seqs.append(seq)\n",
    "            train_df['src_seqs'] = seqs\n",
    "            seqs = []\n",
    "            for sent in tqdm(test_df['question_text'], desc='tokenize'):\n",
    "                seq = tokens2ids(sent.split()[:max_seq_len], vocab['token2id'])\n",
    "                seqs.append(seq)\n",
    "            test_df['src_seqs'] = seqs\n",
    "    if 'embedding_matrix' not in shared_resources:\n",
    "        embedding_matrix = load_embedding(shared_resources['vocab'], max_vocab_size, embed_size)\n",
    "        shared_resources['embedding_matrix'] = embedding_matrix\n",
    "    splits = list(\n",
    "        StratifiedKFold(n_splits=n_folds, shuffle=True).split(train_df['target'], train_df['target']))\n",
    "    scores = []\n",
    "    best_threshold = []\n",
    "    best_threshold_global = None\n",
    "    best_score = -1\n",
    "    predictions_train_reduced = []\n",
    "    targets_train = []\n",
    "    predictions_tes_reduced = np.zeros((len(test_df), n_folds))\n",
    "    predictions_te =  np.zeros((len(test_df),))\n",
    "    for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "        grow_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "        dev_df = train_df.iloc[valid_idx].reset_index(drop=True)\n",
    "        predictions_te_i, predictions_va, targets_va, best_threshold_i = main(grow_df, dev_df, test_df, device,\n",
    "                                                                              **kwargs,\n",
    "                                                                              idx=idx,\n",
    "                                                                              shared_resources=shared_resources,\n",
    "                                                                              return_reduced=True)\n",
    "        # predictions_va_raw shape (#len_va,n_models)\n",
    "        predictions_tes_reduced[:, idx] = predictions_te_i\n",
    "        scores.append([f1_score(targets_va, predictions_va > threshold), roc_auc_score(targets_va, predictions_va)])\n",
    "        best_threshold.append(best_threshold_i)\n",
    "        predictions_te += predictions_te_i / n_folds\n",
    "        predictions_train_reduced.append(predictions_va)\n",
    "        targets_train.append(targets_va)\n",
    "    # calculate model coefficient\n",
    "    coeff = (np.corrcoef(predictions_tes_reduced, rowvar=False).sum() - n_folds) / n_folds / (n_folds - 1)\n",
    "    # create data set for stacking\n",
    "    predictions_train_reduced = np.concatenate(predictions_train_reduced)\n",
    "    targets_train = np.concatenate(targets_train)  # len_train\n",
    "    # train optimal combining weights\n",
    "\n",
    "    # simple average\n",
    "    for t in np.arange(0, 1, 0.01):\n",
    "        score = f1_score(targets_train, predictions_train_reduced > t)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold_global = t\n",
    "    print('avg of best threshold {} macro-f1 best threshold {} best score {}'.format(best_threshold,\n",
    "                                                                                     best_threshold_global, best_score))\n",
    "    return predictions_te, predictions_te, scores, best_threshold_global, coeff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "bb6b6db1b4395cf6372afa0e31edbb4646137be1"
   },
   "outputs": [],
   "source": [
    "#main routine\n",
    "def main(train_df, valid_df, test_df, device=None, epochs=3, fine_tuning_epochs=3, batch_size=512, learning_rate=0.001,\n",
    "         learning_rate_max_offset=0.001, dropout=0.1,\n",
    "         threshold=None,\n",
    "         max_vocab_size=95000, embed_size=300, max_seq_len=70, print_every_step=500, idx=0, shared_resources=None,\n",
    "         return_reduced=True):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:{}\".format(0) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if shared_resources is None:\n",
    "        shared_resources = {}\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    mean_len = AverageMeter()\n",
    "    # build vocab of raw df\n",
    "\n",
    "    if 'vocab' not in shared_resources:\n",
    "        counter = build_counter(chain(train_df['question_text'], test_df['question_text']))\n",
    "        vocab = build_vocab(counter, max_vocab_size=max_vocab_size)\n",
    "    else:\n",
    "        vocab = shared_resources['vocab']\n",
    "    if 'embedding_matrix' not in shared_resources:\n",
    "        embedding_matrix = load_embedding(vocab, max_vocab_size, embed_size)\n",
    "    else:\n",
    "        embedding_matrix = shared_resources['embedding_matrix']\n",
    "    # create test dataset\n",
    "    test_dataset = TextDataset(test_df, vocab=vocab, max_seq_len=max_seq_len)\n",
    "    tb = BucketSampler(test_dataset, test_dataset.get_keys(), batch_size=batch_size,\n",
    "                       shuffle_data=False)\n",
    "    test_iter = DataLoader(dataset=test_dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           sampler=tb,\n",
    "                           # shuffle=False,\n",
    "                           num_workers=0,\n",
    "                           collate_fn=collate_fn)\n",
    "\n",
    "    train_dataset = TextDataset(train_df, vocab=vocab, max_seq_len=max_seq_len)\n",
    "    # keys = train_dataset.get_keys()  # for bucket sorting\n",
    "    valid_dataset = TextDataset(valid_df, vocab=vocab, max_seq_len=max_seq_len)\n",
    "    vb = BucketSampler(valid_dataset, valid_dataset.get_keys(), batch_size=batch_size,\n",
    "                       shuffle_data=False)\n",
    "    valid_index_reverse = vb.get_reverse_indexes()\n",
    "    # init model and optimizers\n",
    "    model = InsincereModel(device, hidden_dim=256, hidden_dim_fc=16, dropout=dropout,\n",
    "                           embedding_matrixs=embedding_matrix,\n",
    "                           vocab_size=len(vocab['token2id']),\n",
    "                           embedding_dim=embed_size, max_seq_len=max_seq_len)\n",
    "    if idx == 0:\n",
    "        print(model)\n",
    "        print('total trainable {}'.format(count_parameters(model)))\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "\n",
    "    # init iterator\n",
    "    train_iter = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            # shuffle=True,\n",
    "                            # sampler=NegativeSubSampler(train_dataset, train_dataset.targets),\n",
    "                            sampler=BucketSampler(train_dataset, train_dataset.get_keys(), bucket_size=batch_size * 20,\n",
    "                                                  batch_size=batch_size),\n",
    "                            num_workers=0,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    valid_iter = DataLoader(dataset=valid_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            sampler=vb,\n",
    "                            # shuffle=False,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    # train model\n",
    "\n",
    "    loss_list = []\n",
    "    global_steps = 0\n",
    "    total_steps = epochs * len(train_iter)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    end = time.time()\n",
    "    predictions_tes = []\n",
    "    predictions_vas = []\n",
    "    n_fge = 0\n",
    "    clr = CyclicLR(optimizer, base_lr=learning_rate, max_lr=learning_rate + learning_rate_max_offset,\n",
    "                   step_size=300, mode='exp_range')\n",
    "    clr.on_train_begin()\n",
    "    fine_tuning_epochs = epochs - fine_tuning_epochs\n",
    "    predictions_te = None\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        fine_tuning = epoch >= fine_tuning_epochs\n",
    "        start_fine_tuning = fine_tuning_epochs == epoch\n",
    "        if start_fine_tuning:\n",
    "            model.enable_learning_embedding()\n",
    "            optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "            # fine tuning embedding layer\n",
    "            global_steps = 0\n",
    "            total_steps = (epochs - fine_tuning_epochs) * len(train_iter)\n",
    "            clr = CyclicLR(optimizer, base_lr=learning_rate, max_lr=learning_rate + learning_rate_max_offset,\n",
    "                           step_size=int(len(train_iter) / 8))\n",
    "            clr.on_train_begin()\n",
    "            predictions_te = np.zeros((len(test_df),))\n",
    "            predictions_va = np.zeros((len(valid_dataset.targets),))\n",
    "        for batch_data in train_iter:\n",
    "            data_time.update(time.time() - end)\n",
    "            qids, src_sents, src_seqs, src_lens, tgts = batch_data\n",
    "            mean_len.update(sum(src_lens))\n",
    "            src_seqs = src_seqs.to(device)\n",
    "            tgts = tgts.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(src_seqs, src_lens, return_logits=True).view(-1)\n",
    "            loss = loss_fn(out, tgts)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.detach().to('cpu').item())\n",
    "\n",
    "            global_steps += 1\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if global_steps % print_every_step == 0:\n",
    "                curr_gpu_memory_usage = get_gpu_memory_usage(device_id=torch.cuda.current_device())\n",
    "                print('Global step: {}/{} Total loss: {:.4f}  Current GPU memory '\n",
    "                      'usage: {} maxlen {} '.format(global_steps, total_steps, avg(loss_list), curr_gpu_memory_usage,\n",
    "                                                    mean_len.avg))\n",
    "                loss_list = []\n",
    "\n",
    "                # print(f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                #      f'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t')\n",
    "            if fine_tuning and global_steps % (2 * clr.step_size) == 0:\n",
    "                predictions_te_tmp2 = eval_model(model, test_iter, device)\n",
    "                predictions_va_tmp2 = eval_model(model, valid_iter, device, valid_index_reverse)\n",
    "                report_perf(valid_dataset, predictions_va_tmp2, threshold, idx, epoch,\n",
    "                            desc='val set mean')\n",
    "                predictions_te = predictions_te * n_fge + (\n",
    "                    predictions_te_tmp2)\n",
    "                predictions_va = predictions_va * n_fge + (\n",
    "                    predictions_va_tmp2)\n",
    "                predictions_te /= n_fge + 1\n",
    "                predictions_va /= n_fge + 1\n",
    "                report_perf(valid_dataset, predictions_va, threshold, idx, epoch\n",
    "                            , desc='val set (fge)')\n",
    "                predictions_tes.append(predictions_te_tmp2.reshape([-1, 1]))\n",
    "                predictions_vas.append(predictions_va_tmp2.reshape([-1, 1]))\n",
    "                n_fge += 1\n",
    "\n",
    "            clr.on_batch_end()\n",
    "        if not fine_tuning:\n",
    "            predictions_va = eval_model(model, valid_iter, device, valid_index_reverse)\n",
    "            report_perf(valid_dataset, predictions_va, threshold, idx, epoch)\n",
    "    # pprint(model.attention1.bias.data.to('cpu'))\n",
    "    # pprint(model.attention2.bias.data.to('cpu'))\n",
    "    # reorder index\n",
    "    if predictions_te is not None:\n",
    "        predictions_te = predictions_te[tb.get_reverse_indexes()]\n",
    "    else:\n",
    "        predictions_te = eval_model(model, test_iter, device, tb.get_reverse_indexes())\n",
    "    best_score = -1\n",
    "    best_threshold = None\n",
    "    for t in np.arange(0, 1, 0.01):\n",
    "        score = f1_score(valid_dataset.targets, predictions_va > t)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = t\n",
    "    print('best threshold on validation set: {:.2f} score {:.4f}'.format(best_threshold, best_score))\n",
    "    if not return_reduced and len(predictions_vas) > 0:\n",
    "        predictions_te = np.concatenate(predictions_tes, axis=1)\n",
    "        predictions_te = predictions_te[tb.get_reverse_indexes(), :]\n",
    "        predictions_va = np.concatenate(predictions_vas, axis=1)\n",
    "\n",
    "    # make predictions\n",
    "    return predictions_te, predictions_va, valid_dataset.targets, best_threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "e8e8b944b555a627d588ab56d843d18385bdb032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocssing 18.934462547302246s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building conuter: 1681928it [00:11, 140238.71it/s]\n",
      "building vocab: 120000it [00:00, 828733.67it/s]\n",
      "tokenize: 100%|██████████| 1306122/1306122 [00:15<00:00, 86189.80it/s] \n",
      "tokenize: 100%|██████████| 375806/375806 [00:04<00:00, 90142.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oov 6264/308/297/761/120002\n",
      "oov 6162/50165/0/0/120002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 375806it [00:00, 651869.25it/s]\n",
      "generate lens: 261225it [00:00, 531373.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InsincereModel(\n",
      "  (embedding_0): Embedding(120002, 300)\n",
      "  (embedding_1): Embedding(120002, 300)\n",
      "  (embedding_mean): Embedding(120002, 300)\n",
      "  (learnable_embedding): Embedding(120002, 300, padding_idx=0)\n",
      "  (spatial_dropout): Dropout2d(p=0.2)\n",
      "  (rnn0): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
      "  (rnn1): GRU(256, 128, batch_first=True, bidirectional=True)\n",
      "  (capsule): Capsule()\n",
      "  (dropout2): Dropout(p=0.3)\n",
      "  (lincaps): Linear(in_features=25, out_features=1, bias=True)\n",
      "  (attention1): Attention(\n",
      "    (attention_fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention2): Attention(\n",
      "    (attention_fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=1025, out_features=16, bias=True)\n",
      "  (norm): LayerNorm(torch.Size([1025]), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.2)\n",
      "  (dropout_linear): Dropout(p=0.1)\n",
      "  (hidden2out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "total trainable 36762931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 1044897it [00:01, 552802.88it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 500/16328 Total loss: 0.1396  Current GPU memory usage: 1374 maxlen 7555.308 \n",
      "Global step: 1000/16328 Total loss: 0.1151  Current GPU memory usage: 1374 maxlen 7559.864 \n",
      "Global step: 1500/16328 Total loss: 0.1088  Current GPU memory usage: 1374 maxlen 7555.432 \n",
      "Global step: 2000/16328 Total loss: 0.1072  Current GPU memory usage: 1374 maxlen 7557.9285 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [01:50<12:50, 110.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 0 val set f1 : 0.6634 auc : 0.9637 margin : 0.0564\n",
      "Global step: 2500/16328 Total loss: 0.1037  Current GPU memory usage: 1374 maxlen 7558.7384 \n",
      "Global step: 3000/16328 Total loss: 0.1027  Current GPU memory usage: 1374 maxlen 7559.904333333333 \n",
      "Global step: 3500/16328 Total loss: 0.1014  Current GPU memory usage: 1374 maxlen 7559.050571428572 \n",
      "Global step: 4000/16328 Total loss: 0.1010  Current GPU memory usage: 1374 maxlen 7558.1075 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [03:41<11:02, 110.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 1 val set f1 : 0.6773 auc : 0.9671 margin : 0.0541\n",
      "Global step: 4500/16328 Total loss: 0.0958  Current GPU memory usage: 1374 maxlen 7554.211555555556 \n",
      "Global step: 5000/16328 Total loss: 0.0956  Current GPU memory usage: 1374 maxlen 7557.3308 \n",
      "Global step: 5500/16328 Total loss: 0.0963  Current GPU memory usage: 1374 maxlen 7558.135636363636 \n",
      "Global step: 6000/16328 Total loss: 0.0958  Current GPU memory usage: 1374 maxlen 7558.583666666666 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [05:32<09:13, 110.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 2 val set f1 : 0.6850 auc : 0.9685 margin : 0.0558\n",
      "Global step: 6500/16328 Total loss: 0.0916  Current GPU memory usage: 1374 maxlen 7557.627230769231 \n",
      "Global step: 7000/16328 Total loss: 0.0910  Current GPU memory usage: 1374 maxlen 7558.000142857143 \n",
      "Global step: 7500/16328 Total loss: 0.0910  Current GPU memory usage: 1374 maxlen 7557.5212 \n",
      "Global step: 8000/16328 Total loss: 0.0918  Current GPU memory usage: 1374 maxlen 7557.510375 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [07:23<07:23, 110.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 3 val set f1 : 0.6787 auc : 0.9695 margin : 0.0540\n",
      "Global step: 8500/16328 Total loss: 0.0877  Current GPU memory usage: 1374 maxlen 7558.384705882353 \n",
      "Global step: 9000/16328 Total loss: 0.0857  Current GPU memory usage: 1374 maxlen 7557.049555555555 \n",
      "Global step: 9500/16328 Total loss: 0.0874  Current GPU memory usage: 1374 maxlen 7556.988842105263 \n",
      "Global step: 10000/16328 Total loss: 0.0876  Current GPU memory usage: 1374 maxlen 7557.8059 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [09:14<05:32, 110.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 4 val set f1 : 0.6874 auc : 0.9699 margin : 0.0545\n",
      "Global step: 10500/16328 Total loss: 0.0841  Current GPU memory usage: 1374 maxlen 7558.084285714286 \n",
      "Global step: 11000/16328 Total loss: 0.0823  Current GPU memory usage: 1374 maxlen 7555.868818181818 \n",
      "Global step: 11500/16328 Total loss: 0.0838  Current GPU memory usage: 1374 maxlen 7558.569217391304 \n",
      "Global step: 12000/16328 Total loss: 0.0838  Current GPU memory usage: 1374 maxlen 7558.248583333333 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [11:06<03:42, 111.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0 epoch 5 val set f1 : 0.6808 auc : 0.9690 margin : 0.0533\n",
      "Global step: 500/4082 Total loss: 0.0817  Current GPU memory usage: 1891 maxlen 7557.673387729484 \n",
      "idx 0 epoch 6 val set mean f1 : 0.6824 auc : 0.9685 margin : 0.0520\n",
      "idx 0 epoch 6 val set (fge) f1 : 0.6824 auc : 0.9685 margin : 0.0520\n",
      "Global step: 1000/4082 Total loss: 0.0836  Current GPU memory usage: 1891 maxlen 7557.647818209271 \n",
      "idx 0 epoch 6 val set mean f1 : 0.6858 auc : 0.9686 margin : 0.0527\n",
      "idx 0 epoch 6 val set (fge) f1 : 0.6899 auc : 0.9700 margin : 0.0523\n",
      "Global step: 1500/4082 Total loss: 0.0856  Current GPU memory usage: 1891 maxlen 7557.5714389640625 \n",
      "idx 0 epoch 6 val set mean f1 : 0.6837 auc : 0.9690 margin : 0.0527\n",
      "idx 0 epoch 6 val set (fge) f1 : 0.6925 auc : 0.9707 margin : 0.0524\n",
      "Global step: 2000/4082 Total loss: 0.0873  Current GPU memory usage: 1891 maxlen 7557.898989189948 \n",
      "idx 0 epoch 6 val set mean f1 : 0.6888 auc : 0.9698 margin : 0.0487\n",
      "idx 0 epoch 6 val set (fge) f1 : 0.6943 auc : 0.9713 margin : 0.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [14:48<02:24, 144.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 2500/4082 Total loss: 0.0692  Current GPU memory usage: 1891 maxlen 7558.092092770921 \n",
      "idx 0 epoch 7 val set mean f1 : 0.6790 auc : 0.9666 margin : 0.0502\n",
      "idx 0 epoch 7 val set (fge) f1 : 0.6950 auc : 0.9713 margin : 0.0512\n",
      "Global step: 3000/4082 Total loss: 0.0692  Current GPU memory usage: 1891 maxlen 7558.122392758756 \n",
      "idx 0 epoch 7 val set mean f1 : 0.6795 auc : 0.9665 margin : 0.0518\n",
      "idx 0 epoch 7 val set (fge) f1 : 0.6952 auc : 0.9714 margin : 0.0513\n",
      "Global step: 3500/4082 Total loss: 0.0729  Current GPU memory usage: 1891 maxlen 7558.388670138448 \n",
      "idx 0 epoch 7 val set mean f1 : 0.6806 auc : 0.9668 margin : 0.0497\n",
      "idx 0 epoch 7 val set (fge) f1 : 0.6975 auc : 0.9714 margin : 0.0511\n",
      "Global step: 4000/4082 Total loss: 0.0752  Current GPU memory usage: 1891 maxlen 7558.051335713406 \n",
      "idx 0 epoch 7 val set mean f1 : 0.6793 auc : 0.9674 margin : 0.0515\n",
      "idx 0 epoch 7 val set (fge) f1 : 0.6972 auc : 0.9714 margin : 0.0511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [18:32<00:00, 168.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold on validation set: 0.35 score 0.6979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 375806it [00:00, 667980.08it/s]\n",
      "generate lens: 261225it [00:00, 542069.32it/s]\n",
      "generate lens: 1044897it [00:01, 554797.57it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 500/16328 Total loss: 0.1398  Current GPU memory usage: 1891 maxlen 7558.452 \n",
      "Global step: 1000/16328 Total loss: 0.1159  Current GPU memory usage: 1891 maxlen 7562.229 \n",
      "Global step: 1500/16328 Total loss: 0.1094  Current GPU memory usage: 1891 maxlen 7561.242666666667 \n",
      "Global step: 2000/16328 Total loss: 0.1084  Current GPU memory usage: 1891 maxlen 7558.8605 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [01:51<13:00, 111.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 0 val set f1 : 0.6554 auc : 0.9648 margin : 0.0567\n",
      "Global step: 2500/16328 Total loss: 0.1040  Current GPU memory usage: 1891 maxlen 7561.014 \n",
      "Global step: 3000/16328 Total loss: 0.1008  Current GPU memory usage: 1891 maxlen 7561.043 \n",
      "Global step: 3500/16328 Total loss: 0.1015  Current GPU memory usage: 1891 maxlen 7555.178857142857 \n",
      "Global step: 4000/16328 Total loss: 0.1016  Current GPU memory usage: 1891 maxlen 7559.97525 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [03:42<11:08, 111.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 1 val set f1 : 0.6838 auc : 0.9682 margin : 0.0558\n",
      "Global step: 4500/16328 Total loss: 0.0970  Current GPU memory usage: 1891 maxlen 7559.31 \n",
      "Global step: 5000/16328 Total loss: 0.0954  Current GPU memory usage: 1891 maxlen 7559.0486 \n",
      "Global step: 5500/16328 Total loss: 0.0956  Current GPU memory usage: 1891 maxlen 7560.981636363636 \n",
      "Global step: 6000/16328 Total loss: 0.0957  Current GPU memory usage: 1891 maxlen 7560.251666666667 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [05:34<09:16, 111.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 2 val set f1 : 0.6865 auc : 0.9703 margin : 0.0583\n",
      "Global step: 6500/16328 Total loss: 0.0917  Current GPU memory usage: 1891 maxlen 7557.930153846154 \n",
      "Global step: 7000/16328 Total loss: 0.0916  Current GPU memory usage: 1891 maxlen 7560.280285714286 \n",
      "Global step: 7500/16328 Total loss: 0.0909  Current GPU memory usage: 1891 maxlen 7559.520933333333 \n",
      "Global step: 8000/16328 Total loss: 0.0905  Current GPU memory usage: 1891 maxlen 7560.793375 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [07:25<07:25, 111.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 3 val set f1 : 0.6860 auc : 0.9698 margin : 0.0556\n",
      "Global step: 8500/16328 Total loss: 0.0876  Current GPU memory usage: 1891 maxlen 7559.288823529412 \n",
      "Global step: 9000/16328 Total loss: 0.0861  Current GPU memory usage: 1891 maxlen 7559.685888888889 \n",
      "Global step: 9500/16328 Total loss: 0.0873  Current GPU memory usage: 1891 maxlen 7557.630631578947 \n",
      "Global step: 10000/16328 Total loss: 0.0870  Current GPU memory usage: 1891 maxlen 7560.3489 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [09:16<05:34, 111.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 4 val set f1 : 0.6912 auc : 0.9710 margin : 0.0513\n",
      "Global step: 10500/16328 Total loss: 0.0828  Current GPU memory usage: 1891 maxlen 7558.223333333333 \n",
      "Global step: 11000/16328 Total loss: 0.0824  Current GPU memory usage: 1891 maxlen 7559.4941818181815 \n",
      "Global step: 11500/16328 Total loss: 0.0831  Current GPU memory usage: 1891 maxlen 7558.728260869565 \n",
      "Global step: 12000/16328 Total loss: 0.0840  Current GPU memory usage: 1891 maxlen 7559.7404166666665 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [11:07<03:42, 111.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 1 epoch 5 val set f1 : 0.6903 auc : 0.9706 margin : 0.0534\n",
      "Global step: 500/4082 Total loss: 0.0818  Current GPU memory usage: 1892 maxlen 7559.809273497568 \n",
      "idx 1 epoch 6 val set mean f1 : 0.6899 auc : 0.9696 margin : 0.0488\n",
      "idx 1 epoch 6 val set (fge) f1 : 0.6899 auc : 0.9696 margin : 0.0488\n",
      "Global step: 1000/4082 Total loss: 0.0846  Current GPU memory usage: 1892 maxlen 7559.967235391817 \n",
      "idx 1 epoch 6 val set mean f1 : 0.6901 auc : 0.9697 margin : 0.0515\n",
      "idx 1 epoch 6 val set (fge) f1 : 0.6973 auc : 0.9711 margin : 0.0501\n",
      "Global step: 1500/4082 Total loss: 0.0848  Current GPU memory usage: 1892 maxlen 7559.830859886512 \n",
      "idx 1 epoch 6 val set mean f1 : 0.6914 auc : 0.9703 margin : 0.0512\n",
      "idx 1 epoch 6 val set (fge) f1 : 0.7012 auc : 0.9720 margin : 0.0505\n",
      "Global step: 2000/4082 Total loss: 0.0861  Current GPU memory usage: 1892 maxlen 7559.622981889654 \n",
      "idx 1 epoch 6 val set mean f1 : 0.6932 auc : 0.9706 margin : 0.0509\n",
      "idx 1 epoch 6 val set (fge) f1 : 0.7027 auc : 0.9725 margin : 0.0506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [14:50<02:24, 144.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 2500/4082 Total loss: 0.0690  Current GPU memory usage: 1892 maxlen 7559.354875898549 \n",
      "idx 1 epoch 7 val set mean f1 : 0.6846 auc : 0.9684 margin : 0.0484\n",
      "idx 1 epoch 7 val set (fge) f1 : 0.7034 auc : 0.9726 margin : 0.0502\n",
      "Global step: 3000/4082 Total loss: 0.0705  Current GPU memory usage: 1892 maxlen 7559.531483667847 \n",
      "idx 1 epoch 7 val set mean f1 : 0.6853 auc : 0.9686 margin : 0.0497\n",
      "idx 1 epoch 7 val set (fge) f1 : 0.7037 auc : 0.9726 margin : 0.0501\n",
      "Global step: 3500/4082 Total loss: 0.0732  Current GPU memory usage: 1892 maxlen 7559.235869427156 \n",
      "idx 1 epoch 7 val set mean f1 : 0.6858 auc : 0.9680 margin : 0.0490\n",
      "idx 1 epoch 7 val set (fge) f1 : 0.7049 auc : 0.9726 margin : 0.0499\n",
      "Global step: 4000/4082 Total loss: 0.0745  Current GPU memory usage: 1892 maxlen 7559.275883294349 \n",
      "idx 1 epoch 7 val set mean f1 : 0.6820 auc : 0.9683 margin : 0.0510\n",
      "idx 1 epoch 7 val set (fge) f1 : 0.7051 auc : 0.9727 margin : 0.0501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [18:34<00:00, 168.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold on validation set: 0.32 score 0.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 375806it [00:00, 659100.98it/s]\n",
      "generate lens: 261224it [00:00, 512095.96it/s]\n",
      "generate lens: 1044898it [00:01, 551131.59it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 500/16328 Total loss: 0.1367  Current GPU memory usage: 1892 maxlen 7560.884 \n",
      "Global step: 1000/16328 Total loss: 0.1146  Current GPU memory usage: 1892 maxlen 7560.271 \n",
      "Global step: 1500/16328 Total loss: 0.1097  Current GPU memory usage: 1892 maxlen 7552.416 \n",
      "Global step: 2000/16328 Total loss: 0.1076  Current GPU memory usage: 1892 maxlen 7552.8585 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [01:51<13:00, 111.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 0 val set f1 : 0.6628 auc : 0.9633 margin : 0.0672\n",
      "Global step: 2500/16328 Total loss: 0.1041  Current GPU memory usage: 1892 maxlen 7555.2724 \n",
      "Global step: 3000/16328 Total loss: 0.1025  Current GPU memory usage: 1892 maxlen 7553.816333333333 \n",
      "Global step: 3500/16328 Total loss: 0.1008  Current GPU memory usage: 1892 maxlen 7553.732 \n",
      "Global step: 4000/16328 Total loss: 0.0998  Current GPU memory usage: 1892 maxlen 7549.6235 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [03:42<11:08, 111.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 1 val set f1 : 0.6844 auc : 0.9672 margin : 0.0571\n",
      "Global step: 4500/16328 Total loss: 0.0968  Current GPU memory usage: 1892 maxlen 7550.959777777778 \n",
      "Global step: 5000/16328 Total loss: 0.0949  Current GPU memory usage: 1892 maxlen 7552.5042 \n",
      "Global step: 5500/16328 Total loss: 0.0972  Current GPU memory usage: 1892 maxlen 7552.246181818182 \n",
      "Global step: 6000/16328 Total loss: 0.0953  Current GPU memory usage: 1892 maxlen 7554.101166666666 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [05:33<09:16, 111.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 2 val set f1 : 0.6889 auc : 0.9688 margin : 0.0547\n",
      "Global step: 6500/16328 Total loss: 0.0920  Current GPU memory usage: 1892 maxlen 7554.217230769231 \n",
      "Global step: 7000/16328 Total loss: 0.0924  Current GPU memory usage: 1892 maxlen 7553.983285714286 \n",
      "Global step: 7500/16328 Total loss: 0.0916  Current GPU memory usage: 1892 maxlen 7553.5232 \n",
      "Global step: 8000/16328 Total loss: 0.0905  Current GPU memory usage: 1892 maxlen 7553.504125 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [07:24<07:25, 111.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 3 val set f1 : 0.6944 auc : 0.9691 margin : 0.0537\n",
      "Global step: 8500/16328 Total loss: 0.0880  Current GPU memory usage: 1892 maxlen 7552.598352941176 \n",
      "Global step: 9000/16328 Total loss: 0.0864  Current GPU memory usage: 1892 maxlen 7553.432222222223 \n",
      "Global step: 9500/16328 Total loss: 0.0862  Current GPU memory usage: 1892 maxlen 7552.822947368421 \n",
      "Global step: 10000/16328 Total loss: 0.0872  Current GPU memory usage: 1892 maxlen 7553.5118 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [09:16<05:33, 111.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 4 val set f1 : 0.6952 auc : 0.9697 margin : 0.0504\n",
      "Global step: 10500/16328 Total loss: 0.0829  Current GPU memory usage: 1892 maxlen 7551.47380952381 \n",
      "Global step: 11000/16328 Total loss: 0.0838  Current GPU memory usage: 1892 maxlen 7552.6246363636365 \n",
      "Global step: 11500/16328 Total loss: 0.0828  Current GPU memory usage: 1892 maxlen 7552.619043478261 \n",
      "Global step: 12000/16328 Total loss: 0.0832  Current GPU memory usage: 1892 maxlen 7551.484833333333 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [11:07<03:42, 111.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 2 epoch 5 val set f1 : 0.6883 auc : 0.9694 margin : 0.0520\n",
      "Global step: 500/4082 Total loss: 0.0819  Current GPU memory usage: 1892 maxlen 7552.282284638318 \n",
      "idx 2 epoch 6 val set mean f1 : 0.6858 auc : 0.9686 margin : 0.0511\n",
      "idx 2 epoch 6 val set (fge) f1 : 0.6858 auc : 0.9686 margin : 0.0511\n",
      "Global step: 1000/4082 Total loss: 0.0835  Current GPU memory usage: 1892 maxlen 7552.454552317681 \n",
      "idx 2 epoch 6 val set mean f1 : 0.6911 auc : 0.9691 margin : 0.0503\n",
      "idx 2 epoch 6 val set (fge) f1 : 0.6955 auc : 0.9705 margin : 0.0507\n",
      "Global step: 1500/4082 Total loss: 0.0850  Current GPU memory usage: 1892 maxlen 7552.1380037829185 \n",
      "idx 2 epoch 6 val set mean f1 : 0.6913 auc : 0.9693 margin : 0.0511\n",
      "idx 2 epoch 6 val set (fge) f1 : 0.7004 auc : 0.9712 margin : 0.0508\n",
      "Global step: 2000/4082 Total loss: 0.0879  Current GPU memory usage: 1892 maxlen 7552.500982731995 \n",
      "idx 2 epoch 6 val set mean f1 : 0.6955 auc : 0.9696 margin : 0.0522\n",
      "idx 2 epoch 6 val set (fge) f1 : 0.7015 auc : 0.9716 margin : 0.0511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [14:50<02:24, 144.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 2500/4082 Total loss: 0.0686  Current GPU memory usage: 1892 maxlen 7552.128645056287 \n",
      "idx 2 epoch 7 val set mean f1 : 0.6838 auc : 0.9675 margin : 0.0485\n",
      "idx 2 epoch 7 val set (fge) f1 : 0.7022 auc : 0.9717 margin : 0.0506\n",
      "Global step: 3000/4082 Total loss: 0.0704  Current GPU memory usage: 1892 maxlen 7552.653482880755 \n",
      "idx 2 epoch 7 val set mean f1 : 0.6861 auc : 0.9676 margin : 0.0486\n",
      "idx 2 epoch 7 val set (fge) f1 : 0.7029 auc : 0.9718 margin : 0.0503\n",
      "Global step: 3500/4082 Total loss: 0.0732  Current GPU memory usage: 1892 maxlen 7552.444938397053 \n",
      "idx 2 epoch 7 val set mean f1 : 0.6846 auc : 0.9673 margin : 0.0514\n",
      "idx 2 epoch 7 val set (fge) f1 : 0.7035 auc : 0.9718 margin : 0.0504\n",
      "Global step: 4000/4082 Total loss: 0.0743  Current GPU memory usage: 1892 maxlen 7552.490336082728 \n",
      "idx 2 epoch 7 val set mean f1 : 0.6845 auc : 0.9671 margin : 0.0538\n",
      "idx 2 epoch 7 val set (fge) f1 : 0.7037 auc : 0.9717 margin : 0.0509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [18:33<00:00, 168.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold on validation set: 0.34 score 0.7048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 375806it [00:00, 628678.45it/s]\n",
      "generate lens: 261224it [00:00, 536819.22it/s]\n",
      "generate lens: 1044898it [00:01, 524318.48it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 500/16328 Total loss: 0.1389  Current GPU memory usage: 1892 maxlen 7542.16 \n",
      "Global step: 1000/16328 Total loss: 0.1136  Current GPU memory usage: 1892 maxlen 7552.615 \n",
      "Global step: 1500/16328 Total loss: 0.1101  Current GPU memory usage: 1892 maxlen 7550.710666666667 \n",
      "Global step: 2000/16328 Total loss: 0.1076  Current GPU memory usage: 1892 maxlen 7550.202 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [01:51<12:59, 111.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 0 val set f1 : 0.6615 auc : 0.9631 margin : 0.0594\n",
      "Global step: 2500/16328 Total loss: 0.1027  Current GPU memory usage: 1892 maxlen 7552.8512 \n",
      "Global step: 3000/16328 Total loss: 0.1033  Current GPU memory usage: 1892 maxlen 7549.687666666667 \n",
      "Global step: 3500/16328 Total loss: 0.1034  Current GPU memory usage: 1892 maxlen 7550.526285714286 \n",
      "Global step: 4000/16328 Total loss: 0.0989  Current GPU memory usage: 1892 maxlen 7550.97325 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [03:42<11:08, 111.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 1 val set f1 : 0.6828 auc : 0.9667 margin : 0.0602\n",
      "Global step: 4500/16328 Total loss: 0.0960  Current GPU memory usage: 1892 maxlen 7551.875333333333 \n",
      "Global step: 5000/16328 Total loss: 0.0958  Current GPU memory usage: 1892 maxlen 7551.2826 \n",
      "Global step: 5500/16328 Total loss: 0.0956  Current GPU memory usage: 1892 maxlen 7548.973818181818 \n",
      "Global step: 6000/16328 Total loss: 0.0963  Current GPU memory usage: 1892 maxlen 7550.0171666666665 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [05:34<09:16, 111.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 2 val set f1 : 0.6893 auc : 0.9686 margin : 0.0501\n",
      "Global step: 6500/16328 Total loss: 0.0921  Current GPU memory usage: 1892 maxlen 7549.584923076923 \n",
      "Global step: 7000/16328 Total loss: 0.0922  Current GPU memory usage: 1892 maxlen 7551.096428571429 \n",
      "Global step: 7500/16328 Total loss: 0.0910  Current GPU memory usage: 1892 maxlen 7549.363333333334 \n",
      "Global step: 8000/16328 Total loss: 0.0913  Current GPU memory usage: 1892 maxlen 7551.2225 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [07:25<07:25, 111.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 3 val set f1 : 0.6878 auc : 0.9691 margin : 0.0539\n",
      "Global step: 8500/16328 Total loss: 0.0868  Current GPU memory usage: 1892 maxlen 7549.5905882352945 \n",
      "Global step: 9000/16328 Total loss: 0.0856  Current GPU memory usage: 1892 maxlen 7548.676222222222 \n",
      "Global step: 9500/16328 Total loss: 0.0876  Current GPU memory usage: 1892 maxlen 7549.413157894737 \n",
      "Global step: 10000/16328 Total loss: 0.0874  Current GPU memory usage: 1892 maxlen 7548.8495 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [09:16<05:33, 111.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 4 val set f1 : 0.6883 auc : 0.9694 margin : 0.0529\n",
      "Global step: 10500/16328 Total loss: 0.0832  Current GPU memory usage: 1892 maxlen 7550.3786666666665 \n",
      "Global step: 11000/16328 Total loss: 0.0835  Current GPU memory usage: 1892 maxlen 7551.063727272727 \n",
      "Global step: 11500/16328 Total loss: 0.0829  Current GPU memory usage: 1892 maxlen 7550.367217391305 \n",
      "Global step: 12000/16328 Total loss: 0.0841  Current GPU memory usage: 1892 maxlen 7549.294833333333 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [11:07<03:42, 111.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 3 epoch 5 val set f1 : 0.6890 auc : 0.9693 margin : 0.0508\n",
      "Global step: 500/4082 Total loss: 0.0819  Current GPU memory usage: 1893 maxlen 7550.141142319159 \n",
      "idx 3 epoch 6 val set mean f1 : 0.6848 auc : 0.9680 margin : 0.0531\n",
      "idx 3 epoch 6 val set (fge) f1 : 0.6848 auc : 0.9680 margin : 0.0531\n",
      "Global step: 1000/4082 Total loss: 0.0837  Current GPU memory usage: 1893 maxlen 7550.431375509588 \n",
      "idx 3 epoch 6 val set mean f1 : 0.6868 auc : 0.9683 margin : 0.0516\n",
      "idx 3 epoch 6 val set (fge) f1 : 0.6930 auc : 0.9697 margin : 0.0524\n",
      "Global step: 1500/4082 Total loss: 0.0864  Current GPU memory usage: 1893 maxlen 7550.469736650662 \n",
      "idx 3 epoch 6 val set mean f1 : 0.6865 auc : 0.9692 margin : 0.0522\n",
      "idx 3 epoch 6 val set (fge) f1 : 0.6958 auc : 0.9707 margin : 0.0523\n",
      "Global step: 2000/4082 Total loss: 0.0873  Current GPU memory usage: 1893 maxlen 7550.176891759091 \n",
      "idx 3 epoch 6 val set mean f1 : 0.6945 auc : 0.9697 margin : 0.0516\n",
      "idx 3 epoch 6 val set (fge) f1 : 0.6990 auc : 0.9713 margin : 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [14:50<02:24, 144.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 2500/4082 Total loss: 0.0690  Current GPU memory usage: 1893 maxlen 7550.340092228401 \n",
      "idx 3 epoch 7 val set mean f1 : 0.6817 auc : 0.9663 margin : 0.0500\n",
      "idx 3 epoch 7 val set (fge) f1 : 0.6998 auc : 0.9714 margin : 0.0517\n",
      "Global step: 3000/4082 Total loss: 0.0704  Current GPU memory usage: 1893 maxlen 7550.638790502427 \n",
      "idx 3 epoch 7 val set mean f1 : 0.6829 auc : 0.9667 margin : 0.0487\n",
      "idx 3 epoch 7 val set (fge) f1 : 0.6990 auc : 0.9714 margin : 0.0512\n",
      "Global step: 3500/4082 Total loss: 0.0727  Current GPU memory usage: 1893 maxlen 7550.77073542487 \n",
      "idx 3 epoch 7 val set mean f1 : 0.6827 auc : 0.9665 margin : 0.0479\n",
      "idx 3 epoch 7 val set (fge) f1 : 0.6991 auc : 0.9714 margin : 0.0507\n",
      "Global step: 4000/4082 Total loss: 0.0743  Current GPU memory usage: 1893 maxlen 7550.3826788132465 \n",
      "idx 3 epoch 7 val set mean f1 : 0.6828 auc : 0.9668 margin : 0.0535\n",
      "idx 3 epoch 7 val set (fge) f1 : 0.6994 auc : 0.9714 margin : 0.0511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [18:34<00:00, 168.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold on validation set: 0.34 score 0.7007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate lens: 375806it [00:00, 630569.73it/s]\n",
      "generate lens: 261224it [00:00, 542757.35it/s]\n",
      "generate lens: 1044898it [00:01, 558659.47it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 500/16328 Total loss: 0.1386  Current GPU memory usage: 1893 maxlen 7551.298 \n",
      "Global step: 1000/16328 Total loss: 0.1138  Current GPU memory usage: 1893 maxlen 7551.34 \n",
      "Global step: 1500/16328 Total loss: 0.1092  Current GPU memory usage: 1893 maxlen 7550.491333333333 \n",
      "Global step: 2000/16328 Total loss: 0.1079  Current GPU memory usage: 1893 maxlen 7554.267 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [01:51<13:01, 111.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 0 val set f1 : 0.6573 auc : 0.9623 margin : 0.0582\n",
      "Global step: 2500/16328 Total loss: 0.1036  Current GPU memory usage: 1893 maxlen 7554.5264 \n",
      "Global step: 3000/16328 Total loss: 0.1027  Current GPU memory usage: 1893 maxlen 7554.539 \n",
      "Global step: 3500/16328 Total loss: 0.0998  Current GPU memory usage: 1893 maxlen 7552.808 \n",
      "Global step: 4000/16328 Total loss: 0.1007  Current GPU memory usage: 1893 maxlen 7554.8835 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [03:42<11:09, 111.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 1 val set f1 : 0.6756 auc : 0.9660 margin : 0.0577\n",
      "Global step: 4500/16328 Total loss: 0.0962  Current GPU memory usage: 1893 maxlen 7555.483333333334 \n",
      "Global step: 5000/16328 Total loss: 0.0943  Current GPU memory usage: 1893 maxlen 7554.1348 \n",
      "Global step: 5500/16328 Total loss: 0.0961  Current GPU memory usage: 1893 maxlen 7555.3789090909095 \n",
      "Global step: 6000/16328 Total loss: 0.0959  Current GPU memory usage: 1893 maxlen 7555.644666666667 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [05:34<09:17, 111.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 2 val set f1 : 0.6816 auc : 0.9680 margin : 0.0521\n",
      "Global step: 6500/16328 Total loss: 0.0911  Current GPU memory usage: 1893 maxlen 7554.718615384615 \n",
      "Global step: 7000/16328 Total loss: 0.0918  Current GPU memory usage: 1893 maxlen 7554.197428571429 \n",
      "Global step: 7500/16328 Total loss: 0.0920  Current GPU memory usage: 1893 maxlen 7553.9 \n",
      "Global step: 8000/16328 Total loss: 0.0903  Current GPU memory usage: 1893 maxlen 7556.113375 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [07:25<07:25, 111.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 3 val set f1 : 0.6811 auc : 0.9672 margin : 0.0509\n",
      "Global step: 8500/16328 Total loss: 0.0872  Current GPU memory usage: 1893 maxlen 7555.6950588235295 \n",
      "Global step: 9000/16328 Total loss: 0.0857  Current GPU memory usage: 1893 maxlen 7554.451 \n",
      "Global step: 9500/16328 Total loss: 0.0863  Current GPU memory usage: 1893 maxlen 7553.9912631578945 \n",
      "Global step: 10000/16328 Total loss: 0.0867  Current GPU memory usage: 1893 maxlen 7553.4212 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [09:16<05:33, 111.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 4 val set f1 : 0.6828 auc : 0.9688 margin : 0.0543\n",
      "Global step: 10500/16328 Total loss: 0.0837  Current GPU memory usage: 1893 maxlen 7555.656095238095 \n",
      "Global step: 11000/16328 Total loss: 0.0817  Current GPU memory usage: 1893 maxlen 7555.474181818182 \n",
      "Global step: 11500/16328 Total loss: 0.0834  Current GPU memory usage: 1893 maxlen 7554.076434782609 \n",
      "Global step: 12000/16328 Total loss: 0.0843  Current GPU memory usage: 1893 maxlen 7556.219166666667 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [11:07<03:42, 111.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 4 epoch 5 val set f1 : 0.6862 auc : 0.9688 margin : 0.0524\n",
      "Global step: 500/4082 Total loss: 0.0813  Current GPU memory usage: 1893 maxlen 7555.19982739683 \n",
      "idx 4 epoch 6 val set mean f1 : 0.6795 auc : 0.9674 margin : 0.0526\n",
      "idx 4 epoch 6 val set (fge) f1 : 0.6795 auc : 0.9674 margin : 0.0526\n",
      "Global step: 1000/4082 Total loss: 0.0837  Current GPU memory usage: 1893 maxlen 7555.173939302431 \n",
      "idx 4 epoch 6 val set mean f1 : 0.6854 auc : 0.9675 margin : 0.0527\n",
      "idx 4 epoch 6 val set (fge) f1 : 0.6885 auc : 0.9689 margin : 0.0526\n",
      "Global step: 1500/4082 Total loss: 0.0859  Current GPU memory usage: 1893 maxlen 7555.0291721227995 \n",
      "idx 4 epoch 6 val set mean f1 : 0.6860 auc : 0.9678 margin : 0.0501\n",
      "idx 4 epoch 6 val set (fge) f1 : 0.6920 auc : 0.9696 margin : 0.0518\n",
      "Global step: 2000/4082 Total loss: 0.0857  Current GPU memory usage: 1893 maxlen 7555.155903411484 \n",
      "idx 4 epoch 6 val set mean f1 : 0.6849 auc : 0.9680 margin : 0.0531\n",
      "idx 4 epoch 6 val set (fge) f1 : 0.6949 auc : 0.9702 margin : 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [14:50<02:24, 144.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 2500/4082 Total loss: 0.0693  Current GPU memory usage: 1893 maxlen 7555.050183100502 \n",
      "idx 4 epoch 7 val set mean f1 : 0.6777 auc : 0.9650 margin : 0.0493\n",
      "idx 4 epoch 7 val set (fge) f1 : 0.6958 auc : 0.9702 margin : 0.0516\n",
      "Global step: 3000/4082 Total loss: 0.0711  Current GPU memory usage: 1893 maxlen 7554.928243473698 \n",
      "idx 4 epoch 7 val set mean f1 : 0.6772 auc : 0.9660 margin : 0.0491\n",
      "idx 4 epoch 7 val set (fge) f1 : 0.6958 auc : 0.9704 margin : 0.0511\n",
      "Global step: 3500/4082 Total loss: 0.0725  Current GPU memory usage: 1893 maxlen 7555.7302807062115 \n",
      "idx 4 epoch 7 val set mean f1 : 0.6777 auc : 0.9653 margin : 0.0508\n",
      "idx 4 epoch 7 val set (fge) f1 : 0.6967 auc : 0.9703 margin : 0.0511\n",
      "Global step: 4000/4082 Total loss: 0.0735  Current GPU memory usage: 1893 maxlen 7555.381755509048 \n",
      "idx 4 epoch 7 val set mean f1 : 0.6777 auc : 0.9659 margin : 0.0516\n",
      "idx 4 epoch 7 val set (fge) f1 : 0.6966 auc : 0.9704 margin : 0.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [18:34<00:00, 168.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold on validation set: 0.32 score 0.6967\n",
      "avg of best threshold [0.35000000000000003, 0.32, 0.34, 0.34, 0.32] macro-f1 best threshold 0.32 best score 0.7007802742990707\n",
      "coeff between predictions 0.9621045053695433\n"
     ]
    }
   ],
   "source": [
    "# seeding\n",
    "set_seed(233)\n",
    "epochs = 8\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "learning_rate_max_offset = 0.002\n",
    "fine_tuning_epochs = 2\n",
    "threshold = 0.31\n",
    "max_vocab_size = 120000\n",
    "embed_size = 300\n",
    "print_every_step = 500\n",
    "max_seq_len = 70\n",
    "share = True\n",
    "dropout = 0.1\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "train_df, test_df = load_data()\n",
    "# shuffling\n",
    "trn_idx = np.random.permutation(len(train_df))\n",
    "train_df = train_df.iloc[trn_idx].reset_index(drop=True)\n",
    "n_folds = 5\n",
    "n_repeats = 1\n",
    "args = {'epochs': epochs, 'batch_size': batch_size, 'learning_rate': learning_rate, 'threshold': threshold,\n",
    "        'max_vocab_size': max_vocab_size,\n",
    "        'embed_size': embed_size, 'print_every_step': print_every_step, 'dropout': dropout,\n",
    "        'learning_rate_max_offset': learning_rate_max_offset,\n",
    "        'fine_tuning_epochs': fine_tuning_epochs, 'max_seq_len': max_seq_len}\n",
    "predictions_te_all = np.zeros((len(test_df),))\n",
    "for _ in range(n_repeats):\n",
    "    if n_folds > 1:\n",
    "        _, predictions_te, _, threshold, coeffs = cv(train_df, test_df, n_folds=n_folds, share=share, **args)\n",
    "        print('coeff between predictions {}'.format(coeffs))\n",
    "    else:\n",
    "        predictions_te, _, _, _ = main(train_df, test_df, test_df, **args)\n",
    "    predictions_te_all += predictions_te / n_repeats\n",
    "sub.prediction = predictions_te_all > threshold\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
