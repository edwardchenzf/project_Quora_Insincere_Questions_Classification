{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "265398a16adabc6afead04691b34432ba4535219"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import unidecode\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import yaml\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec, Doc2Vec, FastText\n",
    "from sklearn import metrics\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe6c2e9b49e682fc479f90934d53dd55a0b14440"
   },
   "source": [
    "# Experiment config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ae3fa4d4bc3b125f0f67bd5e93b6a14a29e8d1f1"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "94d05af05886a1a9e61d892adf29dc184b64225d"
   },
   "outputs": [],
   "source": [
    "modules = \"\"\"\n",
    "class ExperimentConfigBuilder(ExperimentConfigBuilderBase):\n",
    "\n",
    "    default_config = dict(\n",
    "        test=False,\n",
    "        device=0,\n",
    "        maxlen=72,\n",
    "        vocab_mincount=5,\n",
    "        scale_batchsize=[],\n",
    "        validate_from=4,\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def modules(self):\n",
    "        return [\n",
    "            TextNormalizer,\n",
    "            TextTokenizer,\n",
    "            WordEmbeddingFeaturizer,\n",
    "            WordExtraFeaturizer,\n",
    "            SentenceExtraFeaturizer,\n",
    "            Embedding,\n",
    "            Encoder,\n",
    "            Aggregator,\n",
    "            MLP,\n",
    "        ]\n",
    "\n",
    "\n",
    "def build_model(config, embedding_matrix, n_sentence_extra_features):\n",
    "    embedding = Embedding(config, embedding_matrix)\n",
    "    encoder = Encoder(config, embedding.out_size)\n",
    "    aggregator = Aggregator(config)\n",
    "    mlp = MLP(config, encoder.out_size + n_sentence_extra_features)\n",
    "    out = nn.Linear(config.mlp_n_hiddens[-1], 1)\n",
    "    lossfunc = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    return BinaryClassifier(\n",
    "        embedding=embedding,\n",
    "        encoder=encoder,\n",
    "        aggregator=aggregator,\n",
    "        mlp=mlp,\n",
    "        out=out,\n",
    "        lossfunc=lossfunc,\n",
    "    )\n",
    "\n",
    "\n",
    "# =======  Preprocessing modules  =======\n",
    "\n",
    "class TextNormalizer(TextNormalizerPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TextTokenizer(TextTokenizerPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class WordEmbeddingFeaturizer(WordEmbeddingFeaturizerPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class WordExtraFeaturizer(WordExtraFeaturizerPresets):\n",
    "\n",
    "    default_config = dict(\n",
    "        word_extra_features=['idf', 'unk'],\n",
    "    )\n",
    "\n",
    "\n",
    "class SentenceExtraFeaturizer(SentenceExtraFeaturizerPresets):\n",
    "\n",
    "    default_config = dict(\n",
    "        sentence_extra_features=['char', 'word'],\n",
    "    )\n",
    "\n",
    "\n",
    "class Preprocessor(PreprocessorPresets):\n",
    "\n",
    "    embedding_sampling = 400\n",
    "\n",
    "    def build_word_features(self, word_embedding_featurizer,\n",
    "                            embedding_matrices, word_extra_features):\n",
    "        embedding = np.stack(list(embedding_matrices.values()))\n",
    "\n",
    "        # Concat embedding\n",
    "        embedding = np.concatenate(embedding, axis=1)\n",
    "        vocab = word_embedding_featurizer.vocab\n",
    "        embedding[vocab.lfq & vocab.unk] = 0\n",
    "\n",
    "        # Embedding random sampling\n",
    "        n_embed = embedding.shape[1]\n",
    "        n_select = self.embedding_sampling\n",
    "        idx = np.random.permutation(n_embed)[:n_select]\n",
    "        embedding = embedding[:, idx]\n",
    "\n",
    "        word_features = np.concatenate(\n",
    "            [embedding, word_extra_features], axis=1)\n",
    "        return word_features\n",
    "\n",
    "\n",
    "# =======  Training modules  =======\n",
    "\n",
    "class Embedding(EmbeddingPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Encoder(EncoderPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Aggregator(AggregatorPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MLP(MLPPresets):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Ensembler(EnsemblerPresets):\n",
    "    pass\n",
    "\n",
    "\"\"\"\n",
    "os.environ['DATADIR'] = '/kaggle/input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba7ba267545621661b8c3d138729c934c8f5c026"
   },
   "source": [
    "# Library codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8cf738031a390005e553735ecf3df680b62ffa1c"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "5d0230cf7aee7447da70420495de65f0832ef5c1"
   },
   "outputs": [],
   "source": [
    "class ExperimentConfigBuilderBase(metaclass=ABCMeta):\n",
    "\n",
    "    default_config = None\n",
    "\n",
    "    def add_args(self, parser):\n",
    "        parser.add_argument('--modelfile', '-m', type=Path)\n",
    "        parser.add_argument('--outdir-top', type=Path, default=Path('results'))\n",
    "        parser.add_argument('--outdir-bottom', type=str, default='default')\n",
    "        parser.add_argument('--device', '-g', type=int)\n",
    "        parser.add_argument('--test', action='store_true')\n",
    "        parser.add_argument('--logging', action='store_true')\n",
    "        parser.add_argument('--n-rows', type=int)\n",
    "\n",
    "        parser.add_argument('--seed', type=int, default=1029)\n",
    "        parser.add_argument('--optuna-trials', type=int)\n",
    "        parser.add_argument('--gridsearch', action='store_true')\n",
    "        parser.add_argument('--holdout', action='store_true')\n",
    "        parser.add_argument('--cv', type=int, default=5)\n",
    "        parser.add_argument('--cv-part', type=int)\n",
    "        parser.add_argument('--processes', type=int, default=2)\n",
    "\n",
    "        parser.add_argument('--lr', type=float, default=1e-3)\n",
    "        parser.add_argument('--batchsize', type=int, default=512)\n",
    "        parser.add_argument('--batchsize-valid', type=int, default=1024)\n",
    "        parser.add_argument('--scale-batchsize', type=int, nargs='+',\n",
    "                            default=[])\n",
    "        parser.add_argument('--epochs', type=int, default=5)\n",
    "        parser.add_argument('--validate-from', type=int)\n",
    "        parser.add_argument('--pos-weight', type=float, default=1.)\n",
    "        parser.add_argument('--maxlen', type=float, default=72)\n",
    "        parser.add_argument('--vocab-mincount', type=float, default=5)\n",
    "        parser.add_argument('--ensembler-n-snapshots', type=int, default=1)\n",
    "\n",
    "    @abstractmethod\n",
    "    def modules(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def build(self, args=None):\n",
    "        assert self.default_config is not None\n",
    "        parser = argparse.ArgumentParser()\n",
    "        self.add_args(parser)\n",
    "        parser.set_defaults(**self.default_config)\n",
    "\n",
    "        for module in self.modules:\n",
    "            module.add_args(parser)\n",
    "        config, extra_config = parser.parse_known_args(args)\n",
    "\n",
    "        for module in self.modules:\n",
    "            if hasattr(module, 'add_extra_args'):\n",
    "                module.add_extra_args(parser, config)\n",
    "\n",
    "        if config.test:\n",
    "            parser.set_defaults(**dict(\n",
    "                n_rows=500,\n",
    "                batchsize=64,\n",
    "                validate_from=0,\n",
    "                epochs=3,\n",
    "                cv_part=2,\n",
    "                ensembler_test_size=1.,\n",
    "            ))\n",
    "\n",
    "        config = parser.parse_args(args)\n",
    "        if config.modelfile is not None:\n",
    "            config.outdir = config.outdir_top / config.modelfile.stem \\\n",
    "                / config.outdir_bottom\n",
    "        else:\n",
    "            config.outdir = Path('.')\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b8cfaa77fbe7fa86287c63ef12c19b24dbafd92"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "8b341992cf2a89d34489ba87a9151d53c2382296"
   },
   "outputs": [],
   "source": [
    "def load_qiqc(n_rows=None):\n",
    "    train_df = pd.read_csv(f'{os.environ[\"DATADIR\"]}/train.csv', nrows=n_rows)\n",
    "    submit_df = pd.read_csv(f'{os.environ[\"DATADIR\"]}/test.csv', nrows=n_rows)\n",
    "    n_labels = {\n",
    "        0: (train_df.target == 0).sum(),\n",
    "        1: (train_df.target == 1).sum(),\n",
    "    }\n",
    "    train_df['target'] = train_df.target.astype('f')\n",
    "    train_df['weights'] = train_df.target.apply(lambda t: 1 / n_labels[t])\n",
    "\n",
    "    return train_df, submit_df\n",
    "\n",
    "\n",
    "def build_datasets(train_df, submit_df, holdout=False, seed=0):\n",
    "    submit_dataset = QIQCDataset(submit_df)\n",
    "    if holdout:\n",
    "        # Train : Test split for holdout training\n",
    "        splitter = sklearn.model_selection.StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=0.1, random_state=seed)\n",
    "        train_indices, test_indices = list(splitter.split(\n",
    "            train_df, train_df.target))[0]\n",
    "        train_indices.sort(), test_indices.sort()\n",
    "        train_dataset = QIQCDataset(\n",
    "            train_df.iloc[train_indices].reset_index(drop=True))\n",
    "        test_dataset = QIQCDataset(\n",
    "            train_df.iloc[test_indices].reset_index(drop=True))\n",
    "    else:\n",
    "        train_dataset = QIQCDataset(train_df)\n",
    "        test_dataset = QIQCDataset(train_df.head(0))\n",
    "\n",
    "    return train_dataset, test_dataset, submit_dataset\n",
    "\n",
    "\n",
    "class QIQCDataset(object):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    @property\n",
    "    def tokens(self):\n",
    "        return self.df.tokens.values\n",
    "\n",
    "    @tokens.setter\n",
    "    def tokens(self, tokens):\n",
    "        self.df['tokens'] = tokens\n",
    "\n",
    "    @property\n",
    "    def positives(self):\n",
    "        return self.df[self.df.target == 1]\n",
    "\n",
    "    @property\n",
    "    def negatives(self):\n",
    "        return self.df[self.df.target == 0]\n",
    "\n",
    "    def build(self, device):\n",
    "        self._X = self.tids\n",
    "        self.X = torch.Tensor(self._X).type(torch.long).to(device)\n",
    "        if 'target' in self.df:\n",
    "            self._t = self.df.target[:, None]\n",
    "            self._W = self.df.weights\n",
    "            self.t = torch.Tensor(self._t).type(torch.float).to(device)\n",
    "            self.W = torch.Tensor(self._W).type(torch.float).to(device)\n",
    "        if hasattr(self, '_X2'):\n",
    "            self.X2 = torch.Tensor(self._X2).type(torch.float).to(device)\n",
    "        else:\n",
    "            self._X2 = np.zeros((self._X.shape[0], 1), 'f')\n",
    "            self.X2 = torch.Tensor(self._X2).type(torch.float).to(device)\n",
    "\n",
    "    def build_labeled_dataset(self, indices):\n",
    "        return torch.utils.data.TensorDataset(\n",
    "            self.X[indices], self.X2[indices],\n",
    "            self.t[indices], self.W[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74df4b02f3a1e9428d7df3f874fa1d4d829e546d"
   },
   "source": [
    "## Registries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "d020317f35e394340ecf22e2fd3eee33cf96212e"
   },
   "outputs": [],
   "source": [
    "# Registries for preprocessing\n",
    "NORMALIZER_REGISTRY = {}\n",
    "TOKENIZER_REGISTRY = {}\n",
    "WORD_EMBEDDING_FEATURIZER_REGISTRY = {}\n",
    "WORD_EXTRA_FEATURIZER_REGISTRY = {}\n",
    "SENTENCE_EXTRA_FEATURIZER_REGISTRY = {}\n",
    "\n",
    "# Registries for training\n",
    "ENCODER_REGISTRY = {}\n",
    "AGGREGATOR_REGISTRY = {}\n",
    "ATTENTION_REGISTRY = {}\n",
    "\n",
    "\n",
    "def register_preprocessor(name):\n",
    "    def register_cls(cls):\n",
    "        NORMALIZER_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return register_cls\n",
    "\n",
    "\n",
    "def register_tokenizer(name):\n",
    "    def register_cls(cls):\n",
    "        TOKENIZER_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return register_cls\n",
    "\n",
    "\n",
    "def register_word_embedding_features(name):\n",
    "    def register_cls(cls):\n",
    "        WORD_EMBEDDING_FEATURIZER_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return register_cls\n",
    "\n",
    "\n",
    "def register_word_extra_features(name):\n",
    "    def register_cls(cls):\n",
    "        WORD_EXTRA_FEATURIZER_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return register_cls\n",
    "\n",
    "\n",
    "def register_sentence_extra_features(name):\n",
    "    def register_cls(cls):\n",
    "        SENTENCE_EXTRA_FEATURIZER_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return register_cls\n",
    "\n",
    "\n",
    "def register_encoder(name):\n",
    "    def register_cls(cls):\n",
    "        ENCODER_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return register_cls\n",
    "\n",
    "\n",
    "def register_aggregator(name):\n",
    "    def register_cls(cls):\n",
    "        AGGREGATOR_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return register_cls\n",
    "\n",
    "\n",
    "def register_attention(name):\n",
    "    def register_cls(cls):\n",
    "        ATTENTION_REGISTRY[name] = cls\n",
    "        return cls\n",
    "    return register_cls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58e2e5ef76e10157fe4c8c370e324a619cd81da0"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "389e6c38dac0851c117d3b5469d3919e82626ff4"
   },
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "2383c175f33dda038af10fff62718faaf66b03e2"
   },
   "outputs": [],
   "source": [
    "class WordVocab(object):\n",
    "\n",
    "    def __init__(self, mincount=1):\n",
    "        self.counter = Counter()\n",
    "        self.n_documents = 0\n",
    "        self._counters = {}\n",
    "        self._n_documents = defaultdict(int)\n",
    "        self.mincount = mincount\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token2id)\n",
    "\n",
    "    def add_documents(self, documents, name):\n",
    "        self._counters[name] = Counter()\n",
    "        for document in documents:\n",
    "            bow = dict.fromkeys(document, 1)\n",
    "            self._counters[name].update(bow)\n",
    "            self.counter.update(bow)\n",
    "            self.n_documents += 1\n",
    "            self._n_documents[name] += 1\n",
    "\n",
    "    def build(self):\n",
    "        counter = dict(self.counter.most_common())\n",
    "        self.word_freq = {\n",
    "            **{'<PAD>': 0},\n",
    "            **counter,\n",
    "        }\n",
    "        self.token2id = {\n",
    "            **{'<PAD>': 0},\n",
    "            **{word: i + 1 for i, word in enumerate(counter)}\n",
    "        }\n",
    "        self.lfq = np.array(list(self.word_freq.values())) < self.mincount\n",
    "        self.hfq = ~self.lfq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a6921c6d0603ea19e563548a54798f2a4880d51"
   },
   "source": [
    "### Normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "31efade437cb270c0ea53e65bef25064feb51901"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "\n",
    "cdef class StringReplacer:\n",
    "    cpdef public dict rule\n",
    "    cpdef list keys\n",
    "    cpdef list values\n",
    "    cpdef int n_rules\n",
    "\n",
    "    def __init__(self, dict rule):\n",
    "        self.rule = rule\n",
    "        self.keys = list(rule.keys())\n",
    "        self.values = list(rule.values())\n",
    "        self.n_rules = len(rule)\n",
    "\n",
    "    def __call__(self, str x):\n",
    "        cdef int i\n",
    "        for i in range(self.n_rules):\n",
    "            if self.keys[i] in x:\n",
    "                x = x.replace(self.keys[i], self.values[i])\n",
    "        return x\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return (self.rule, self.keys, self.values, self.n_rules)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.rule, self.keys, self.values, self.n_rules = state\n",
    "\n",
    "\n",
    "cdef class RegExpReplacer:\n",
    "    cdef dict rule\n",
    "    cdef list keys\n",
    "    cdef list values\n",
    "    cdef regexp\n",
    "    cdef int n_rules\n",
    "\n",
    "    def __init__(self, dict rule):\n",
    "        self.rule = rule\n",
    "        self.keys = list(rule.keys())\n",
    "        self.values = list(rule.values())\n",
    "        self.regexp = re.compile('(%s)' % '|'.join(self.keys))\n",
    "        self.n_rules = len(rule)\n",
    "\n",
    "    @property\n",
    "    def rule(self):\n",
    "        return self.rule\n",
    "\n",
    "    def __call__(self, str x):\n",
    "        def replace(match):\n",
    "            x = match.group(0)\n",
    "            if x in self.rule:\n",
    "                return self.rule[x]\n",
    "            else:\n",
    "                for i in range(self.n_rules):\n",
    "                    x = re.sub(self.keys[i], self.values[i], x)\n",
    "                return x\n",
    "        return self.regexp.sub(replace, x)\n",
    "\n",
    "\n",
    "cpdef str cylower(str x):\n",
    "    return x.lower()\n",
    "\n",
    "\n",
    "Cache = {}\n",
    "is_alphabet = re.compile(r'[a-zA-Z]')\n",
    "\n",
    "\n",
    "cpdef str unidecode_weak(str string):\n",
    "    \"\"\"Transliterate an Unicode object into an ASCII string\n",
    "    >>> unidecode(u\"\\u5317\\u4EB0\")\n",
    "    \"Bei Jing \"\n",
    "    \"\"\"\n",
    "\n",
    "    cdef list retval = []\n",
    "    cdef int i = 0\n",
    "    cdef int n = len(string)\n",
    "    cdef str char\n",
    "\n",
    "    for i in range(n):\n",
    "        char = string[i]\n",
    "        codepoint = ord(char)\n",
    "\n",
    "        if codepoint < 0x80: # Basic ASCII\n",
    "            retval.append(char)\n",
    "            continue\n",
    "\n",
    "        if codepoint > 0xeffff:\n",
    "            continue  # Characters in Private Use Area and above are ignored\n",
    "\n",
    "        section = codepoint >> 8   # Chop off the last two hex digits\n",
    "        position = codepoint % 256 # Last two hex digits\n",
    "\n",
    "        try:\n",
    "            table = Cache[section]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                mod = __import__('unidecode.x%03x'%(section), [], [], ['data'])\n",
    "            except ImportError:\n",
    "                Cache[section] = None\n",
    "                continue   # No match: ignore this character and carry on.\n",
    "\n",
    "            Cache[section] = table = mod.data\n",
    "\n",
    "        if table and len(table) > position:\n",
    "            if table[position] == '[?]' or is_alphabet.match(table[position]):\n",
    "                retval.append(' ' + char + ' ')\n",
    "            else:\n",
    "                retval.append(table[position])\n",
    "\n",
    "    return ''.join(retval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "b69a0a7f3deea6934905660cf75441b31ac0ac54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KerasFilterReplacer at 0x7f657149daf0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PunctSpacer(StringReplacer):\n",
    "\n",
    "    def __init__(self, edge_only=False):\n",
    "        puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', '█', '½', '…', '“', '★', '”', '–', '●', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', '¯', '♦', '¤', '▲', '¸', '¾', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]  # NOQA\n",
    "        if edge_only:\n",
    "            rule = {\n",
    "                **dict([(f' {p}', f' {p} ') for p in puncts]),\n",
    "                **dict([(f'{p} ', f' {p} ') for p in puncts]),\n",
    "            }\n",
    "        else:\n",
    "            rule = dict([(p, f' {p} ') for p in puncts])\n",
    "        super().__init__(rule)\n",
    "\n",
    "\n",
    "class NumberReplacer(RegExpReplacer):\n",
    "\n",
    "    def __init__(self, with_underscore=False):\n",
    "        prefix, suffix = '', ''\n",
    "        if with_underscore:\n",
    "            prefix += ' __'\n",
    "            suffix = '__ '\n",
    "        rule = {\n",
    "            '[0-9]{5,}': f'{prefix}#####{suffix}',\n",
    "            '[0-9]{4}': f'{prefix}####{suffix}',\n",
    "            '[0-9]{3}': f'{prefix}###{suffix}',\n",
    "            '[0-9]{2}': f'{prefix}##{suffix}',\n",
    "        }\n",
    "        super().__init__(rule)\n",
    "\n",
    "\n",
    "class KerasFilterReplacer(StringReplacer):\n",
    "\n",
    "    def __init__(self):\n",
    "        filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "        rule = dict([(f, ' ') for f in filters])\n",
    "        super().__init__(rule)\n",
    "\n",
    "\n",
    "class MisspellReplacer(StringReplacer):\n",
    "\n",
    "    def __init__(self):\n",
    "        rule = {\n",
    "            \"ain't\": \"is not\",\n",
    "            \"aren't\": \"are not\",\n",
    "            \"can't\": \"cannot\",\n",
    "            \"'cause\": \"because\",\n",
    "            \"could've\": \"could have\",\n",
    "            \"couldn't\": \"could not\",\n",
    "            \"didn't\": \"did not\",\n",
    "            \"doesn't\": \"does not\",\n",
    "            \"don't\": \"do not\",\n",
    "            \"hadn't\": \"had not\",\n",
    "            \"hasn't\": \"has not\",\n",
    "            \"haven't\": \"have not\",\n",
    "            \"he'd\": \"he would\",\n",
    "            \"he'll\": \"he will\",\n",
    "            \"he's\": \"he is\",\n",
    "            \"how'd'y\": \"how do you\",\n",
    "            \"how'd\": \"how did\",\n",
    "            \"how'll\": \"how will\",\n",
    "            \"how's\": \"how is\",\n",
    "            \"i'd've\": \"i would have\",\n",
    "            \"i'd\": \"i would\",\n",
    "            \"i'll've\": \"i will have\",\n",
    "            \"i'll\": \"i will\",\n",
    "            \"i'm\": \"i am\",\n",
    "            \"i've\": \"i have\",\n",
    "            \"isn't\": \"is not\",\n",
    "            \"it'd've\": \"it would have\",\n",
    "            \"it'd\": \"it would\",\n",
    "            \"it'll've\": \"it will have\",\n",
    "            \"it'll\": \"it will\",\n",
    "            \"it's\": \"it is\",\n",
    "            \"let's\": \"let us\",\n",
    "            \"ma'am\": \"madam\",\n",
    "            \"mayn't\": \"may not\",\n",
    "            \"might've\": \"might have\",\n",
    "            \"mightn't've\": \"might not have\",\n",
    "            \"mightn't\": \"might not\",\n",
    "            \"must've\": \"must have\",\n",
    "            \"mustn't've\": \"must not have\",\n",
    "            \"mustn't\": \"must not\",\n",
    "            \"needn't've\": \"need not have\",\n",
    "            \"needn't\": \"need not\",\n",
    "            \"o'clock\": \"of the clock\",\n",
    "            \"oughtn't've\": \"ought not have\",\n",
    "            \"oughtn't\": \"ought not\",\n",
    "            \"shan't've\": \"shall not have\",\n",
    "            \"shan't\": \"shall not\",\n",
    "            \"sha'n't\": \"shall not\",\n",
    "            \"she'd've\": \"she would have\",\n",
    "            \"she'd\": \"she would\",\n",
    "            \"she'll've\": \"she will have\",\n",
    "            \"she'll\": \"she will\",\n",
    "            \"she's\": \"she is\",\n",
    "            \"should've\": \"should have\",\n",
    "            \"shouldn't've\": \"should not have\",\n",
    "            \"shouldn't\": \"should not\",\n",
    "            \"so've\": \"so have\",\n",
    "            \"so's\": \"so as\",\n",
    "            \"this's\": \"this is\",\n",
    "            \"that'd've\": \"that would have\",\n",
    "            \"that'd\": \"that would\",\n",
    "            \"that's\": \"that is\",\n",
    "            \"there'd've\": \"there would have\",\n",
    "            \"there'd\": \"there would\",\n",
    "            \"there's\": \"there is\",\n",
    "            \"here's\": \"here is\",\n",
    "            \"they'd've\": \"they would have\",\n",
    "            \"they'd\": \"they would\",\n",
    "            \"they'll've\": \"they will have\",\n",
    "            \"they'll\": \"they will\",\n",
    "            \"they're\": \"they are\",\n",
    "            \"they've\": \"they have\",\n",
    "            \"to've\": \"to have\",\n",
    "            \"wasn't\": \"was not\",\n",
    "            \"we'd've\": \"we would have\",\n",
    "            \"we'd\": \"we would\",\n",
    "            \"we'll've\": \"we will have\",\n",
    "            \"we'll\": \"we will\",\n",
    "            \"we're\": \"we are\",\n",
    "            \"we've\": \"we have\",\n",
    "            \"weren't\": \"were not\",\n",
    "            \"what'll've\": \"what will have\",\n",
    "            \"what'll\": \"what will\",\n",
    "            \"what're\": \"what are\",\n",
    "            \"what's\": \"what is\",\n",
    "            \"what've\": \"what have\",\n",
    "            \"when's\": \"when is\",\n",
    "            \"when've\": \"when have\",\n",
    "            \"where'd\": \"where did\",\n",
    "            \"where's\": \"where is\",\n",
    "            \"where've\": \"where have\",\n",
    "            \"who'll've\": \"who will have\",\n",
    "            \"who'll\": \"who will\",\n",
    "            \"who's\": \"who is\",\n",
    "            \"who've\": \"who have\",\n",
    "            \"why's\": \"why is\",\n",
    "            \"why've\": \"why have\",\n",
    "            \"will've\": \"will have\",\n",
    "            \"won't've\": \"will not have\",\n",
    "            \"won't\": \"will not\",\n",
    "            \"would've\": \"would have\",\n",
    "            \"wouldn't've\": \"would not have\",\n",
    "            \"wouldn't\": \"would not\",\n",
    "            \"y'all'd've\": \"you all would have\",\n",
    "            \"y'all'd\": \"you all would\",\n",
    "            \"y'all're\": \"you all are\",\n",
    "            \"y'all've\": \"you all have\",\n",
    "            \"y'all\": \"you all\",\n",
    "            \"you'd've\": \"you would have\",\n",
    "            \"you'd\": \"you would\",\n",
    "            \"you'll've\": \"you will have\",\n",
    "            \"you'll\": \"you will\",\n",
    "            \"you're\": \"you are\",\n",
    "            \"you've\": \"you have\",\n",
    "            \"colour\": \"color\",\n",
    "            \"centre\": \"center\",\n",
    "            \"favourite\": \"favorite\",\n",
    "            \"travelling\": \"traveling\",\n",
    "            \"counselling\": \"counseling\",\n",
    "            \"theatre\": \"theater\",\n",
    "            \"cancelled\": \"canceled\",\n",
    "            \"labour\": \"labor\",\n",
    "            \"organisation\": \"organization\",\n",
    "            \"wwii\": \"world war 2\",\n",
    "            \"citicise\": \"criticize\",\n",
    "            \"youtu \": \"youtube \",\n",
    "            \"qoura\": \"quora\",\n",
    "            \"sallary\": \"salary\",\n",
    "            \"whta\": \"what\",\n",
    "            \"narcisist\": \"narcissist\",\n",
    "            \"howdo\": \"how do\",\n",
    "            \"whatare\": \"what are\",\n",
    "            \"howcan\": \"how can\",\n",
    "            \"howmuch\": \"how much\",\n",
    "            \"howmany\": \"how many\",\n",
    "            \"whydo\": \"why do\",\n",
    "            \"doi\": \"do i\",\n",
    "            \"thebest\": \"the best\",\n",
    "            \"howdoes\": \"how does\",\n",
    "            \"mastrubation\": \"masturbation\",\n",
    "            \"mastrubate\": \"masturbate\",\n",
    "            \"mastrubating\": \"masturbating\",\n",
    "            \"pennis\": \"penis\",\n",
    "            \"etherium\": \"ethereum\",\n",
    "            \"narcissit\": \"narcissist\",\n",
    "            \"bigdata\": \"big data\",\n",
    "            \"2k17\": \"2017\",\n",
    "            \"2k18\": \"2018\",\n",
    "            \"qouta\": \"quota\",\n",
    "            \"exboyfriend\": \"ex boyfriend\",\n",
    "            \"airhostess\": \"air hostess\",\n",
    "            \"whst\": \"what\",\n",
    "            \"watsapp\": \"whatsapp\",\n",
    "            \"demonitisation\": \"demonetization\",\n",
    "            \"demonitization\": \"demonetization\",\n",
    "            \"demonetisation\": \"demonetization\",\n",
    "        }\n",
    "        super().__init__(rule)\n",
    "\n",
    "\n",
    "register_preprocessor('lower')(cylower)\n",
    "register_preprocessor('punct')(PunctSpacer())\n",
    "register_preprocessor('unidecode')(unidecode)\n",
    "register_preprocessor('unidecode_weak')(unidecode_weak)\n",
    "register_preprocessor('number')(NumberReplacer())\n",
    "register_preprocessor('number+underscore')(\n",
    "    NumberReplacer(with_underscore=True))\n",
    "register_preprocessor('misspell')(MisspellReplacer())\n",
    "register_preprocessor('keras')(KerasFilterReplacer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b377e5831c2d83a883355019ab9f488e7b6e4f66"
   },
   "source": [
    "### Featurizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "584c36f099b5d69b5291c962cb4c4196eaa75b7e"
   },
   "outputs": [],
   "source": [
    "def load_pretrained_vectors(names, token2id, test=False):\n",
    "    assert isinstance(names, list)\n",
    "    with Pool(processes=len(names)) as pool:\n",
    "        f = partial(load_pretrained_vector, token2id=token2id, test=test)\n",
    "        vectors = pool.map(f, names)\n",
    "    return dict([(n, v) for n, v in zip(names, vectors)])\n",
    "\n",
    "\n",
    "def load_pretrained_vector(name, token2id, test=False):\n",
    "    loader = dict(\n",
    "        gnews=GNewsPretrainedVector,\n",
    "        wnews=WNewsPretrainedVector,\n",
    "        paragram=ParagramPretrainedVector,\n",
    "        glove=GlovePretrainedVector,\n",
    "    )\n",
    "    return loader[name].load(token2id, test)\n",
    "\n",
    "\n",
    "class BasePretrainedVector(object):\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, token2id, test=False, limit=None):\n",
    "        embed_shape = (len(token2id), 300)\n",
    "        freqs = np.zeros((len(token2id)), dtype='f')\n",
    "\n",
    "        if test:\n",
    "            np.random.seed(0)\n",
    "            vectors = np.random.normal(0, 1, embed_shape)\n",
    "            vectors[0] = 0\n",
    "            vectors[len(token2id) // 2:] = 0\n",
    "        else:\n",
    "            vectors = np.zeros(embed_shape, dtype='f')\n",
    "            path = f'{os.environ[\"DATADIR\"]}/{cls.path}'\n",
    "            for i, o in enumerate(\n",
    "                    open(path, encoding=\"utf8\", errors='ignore')):\n",
    "                token, *vector = o.split(' ')\n",
    "                token = str.lower(token)\n",
    "                if token not in token2id or len(o) <= 100:\n",
    "                    continue\n",
    "                if limit is not None and i > limit:\n",
    "                    break\n",
    "                freqs[token2id[token]] += 1\n",
    "                vectors[token2id[token]] += np.array(vector, 'f')\n",
    "\n",
    "        vectors[freqs != 0] /= freqs[freqs != 0][:, None]\n",
    "        vec = KeyedVectors(300)\n",
    "        vec.add(list(token2id.keys()), vectors, replace=True)\n",
    "\n",
    "        return vec\n",
    "\n",
    "\n",
    "class GNewsPretrainedVector(object):\n",
    "\n",
    "    name = 'GoogleNews-vectors-negative300'\n",
    "    path = f'embeddings/{name}/{name}.bin'\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, tokens, limit=None):\n",
    "        raise NotImplementedError\n",
    "        path = f'{os.environ[\"DATADIR\"]}/{cls.path}'\n",
    "        return KeyedVectors.load_word2vec_format(\n",
    "            path, binary=True, limit=limit)\n",
    "\n",
    "\n",
    "class WNewsPretrainedVector(BasePretrainedVector):\n",
    "\n",
    "    name = 'wiki-news-300d-1M'\n",
    "    path = f'embeddings/{name}/{name}.vec'\n",
    "\n",
    "\n",
    "class ParagramPretrainedVector(BasePretrainedVector):\n",
    "\n",
    "    name = 'paragram_300_sl999'\n",
    "    path = f'embeddings/{name}/{name}.txt'\n",
    "\n",
    "\n",
    "class GlovePretrainedVector(BasePretrainedVector):\n",
    "\n",
    "    name = 'glove.840B.300d'\n",
    "    path = f'embeddings/{name}/{name}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5213478751cb87cfb53075e6241029f7e64bf132"
   },
   "outputs": [],
   "source": [
    "@register_word_embedding_features('pretrained')\n",
    "class PretrainedVectorFeaturizer(object):\n",
    "\n",
    "    def __init__(self, config, vocab):\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(self, parser):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, features, datasets):\n",
    "        # Nothing to do\n",
    "        return features\n",
    "\n",
    "\n",
    "class Any2VecFeaturizer(object):\n",
    "\n",
    "    def __init__(self, config, vocab):\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def build_fillvalue(self, mode, initialW):\n",
    "        n_embed = initialW.shape[1]\n",
    "        n_fill = initialW[self.vocab.unk].shape\n",
    "        assert mode in {'zeros', 'mean', 'noise'}\n",
    "        if mode == 'zeros':\n",
    "            return np.zeros(n_embed, 'f')\n",
    "        elif mode == 'mean':\n",
    "            return initialW.mean(axis=0)\n",
    "        elif mode == 'noise':\n",
    "            mean, std = initialW.mean(), initialW.std()\n",
    "            return np.random.normal(mean, std, (n_fill, n_embed))\n",
    "\n",
    "    def __call__(self, features, datasets):\n",
    "        tokens = np.concatenate([d.tokens for d in datasets])\n",
    "        model = self.build_model()\n",
    "        model.build_vocab_from_freq(self.vocab.word_freq)\n",
    "        initialW = features.copy()\n",
    "        initialW[self.vocab.unk] = self.build_fillvalue(\n",
    "            self.config.finetune_word2vec_init_unk, initialW)\n",
    "        idxmap = np.array(\n",
    "            [self.vocab.token2id[w] for w in model.wv.index2entity])\n",
    "        model = self.initialize(model, initialW, idxmap)\n",
    "        model.train(tokens, total_examples=len(tokens), epochs=model.epochs)\n",
    "        finetunedW = np.zeros((initialW.shape), 'f')\n",
    "        for i, word in enumerate(self.vocab.token2id):\n",
    "            if word in model.wv:\n",
    "                finetunedW[i] = model.wv.get_vector(word)\n",
    "        return finetunedW\n",
    "\n",
    "\n",
    "@register_word_embedding_features('word2vec')\n",
    "class Word2VecFeaturizer(Any2VecFeaturizer):\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(self, parser):\n",
    "        parser.add_argument('--finetune-word2vec-init-unk', type=str,\n",
    "                            choices=['zeros', 'mean', 'noise'])\n",
    "        parser.add_argument('--finetune-word2vec-mincount', type=int)\n",
    "        parser.add_argument('--finetune-word2vec-workers', type=int)\n",
    "        parser.add_argument('--finetune-word2vec-iter', type=int)\n",
    "        parser.add_argument('--finetune-word2vec-size', type=int)\n",
    "        parser.add_argument('--finetune-word2vec-window', type=int, default=5)\n",
    "        parser.add_argument('--finetune-word2vec-sorted-vocab', type=int,\n",
    "                            default=0)\n",
    "        parser.add_argument('--finetune-word2vec-sg', type=int, choices=[0, 1])\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Word2Vec(\n",
    "            min_count=self.config.finetune_word2vec_mincount,\n",
    "            workers=self.config.finetune_word2vec_workers,\n",
    "            iter=self.config.finetune_word2vec_iter,\n",
    "            size=self.config.finetune_word2vec_size,\n",
    "            window=self.config.finetune_word2vec_window,\n",
    "            sg=self.config.finetune_word2vec_sg,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def initialize(self, model, initialW, idxmap):\n",
    "        model.wv.vectors[:] = initialW[idxmap]\n",
    "        model.trainables.syn1neg[:] = initialW[idxmap]\n",
    "        return model\n",
    "\n",
    "\n",
    "@register_word_embedding_features('fasttext')\n",
    "class FastTextFeaturizer(Any2VecFeaturizer):\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(self, parser):\n",
    "        parser.add_argument('--finetune-fasttext-init-unk', type=str,\n",
    "                            choices=['zeros', 'mean', 'noise'])\n",
    "        parser.add_argument('--finetune-fasttext-mincount', type=int)\n",
    "        parser.add_argument('--finetune-fasttext-workers', type=int)\n",
    "        parser.add_argument('--finetune-fasttext-iter', type=int)\n",
    "        parser.add_argument('--finetune-fasttext-size', type=int)\n",
    "        parser.add_argument('--finetune-fasttext-sg', type=int, choices=[0, 1])\n",
    "        parser.add_argument('--finetune-fasttext-min_n', type=int)\n",
    "        parser.add_argument('--finetune-fasttext-max_n', type=int)\n",
    "\n",
    "    def build_model(self):\n",
    "        model = FastText(\n",
    "            min_count=self.config.finetune_fasttext_mincount,\n",
    "            workers=self.config.finetune_fasttext_workers,\n",
    "            iter=self.config.finetune_fasttext_iter,\n",
    "            size=self.config.finetune_fasttext_size,\n",
    "            sg=self.config.finetune_fasttext_sg,\n",
    "            min_n=self.config.finetune_fasttext_min_n,\n",
    "            max_n=self.config.finetune_fasttext_max_n,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def initialize(self, model, initialW, idxmap):\n",
    "        model.wv.vectors[:] = initialW[idxmap]\n",
    "        model.wv.vectors_vocab[:] = initialW[idxmap]\n",
    "        model.trainables.syn1neg[:] = initialW[idxmap]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "08f65c6569b616b1968600bc25fb71fdce3d4b01"
   },
   "outputs": [],
   "source": [
    "@register_word_extra_features('idf')\n",
    "class IDFWordFeaturizer(object):\n",
    "\n",
    "    def __call__(self, vocab):\n",
    "        dfs = np.array(list(vocab.word_freq.values()))\n",
    "        dfs[0] = vocab.n_documents\n",
    "        features = np.log(vocab.n_documents / dfs)\n",
    "        features = features[:, None]\n",
    "        return features\n",
    "\n",
    "\n",
    "@register_word_extra_features('unk')\n",
    "class UnkWordFeaturizer(object):\n",
    "\n",
    "    def __call__(self, vocab):\n",
    "        features = vocab.unk.astype('f')\n",
    "        features[0] = 0\n",
    "        features = features[:, None]\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "46357e517227107e6279b9a1fab2343ee40a9f03"
   },
   "outputs": [],
   "source": [
    "@register_sentence_extra_features('char')\n",
    "class CharacterStatisticsFeaturizer(object):\n",
    "\n",
    "    n_dims = 3\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        feature = {}\n",
    "        feature['n_chars'] = len(sentence)\n",
    "        feature['n_caps'] = sum(1 for char in sentence if char.isupper())\n",
    "        feature['caps_rate'] = feature['n_caps'] / feature['n_chars']\n",
    "        features = np.array(list(feature.values()))\n",
    "        return features\n",
    "\n",
    "\n",
    "@register_sentence_extra_features('word')\n",
    "class WordStatisticsFeaturizer(object):\n",
    "\n",
    "    n_dims = 3\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        feature = {}\n",
    "        tokens = sentence.split()\n",
    "        feature['n_words'] = len(tokens)\n",
    "        feature['unique_words'] = len(set(tokens))\n",
    "        feature['unique_rate'] = feature['unique_words'] / feature['n_words']\n",
    "        features = np.array(list(feature.values()))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b27798fd45971455def10ff158d3fe11f9c8f3d7"
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "3475624974a1703aabe270e48cbbe6ac60dc3c31"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "cpdef list cysplit(str x):\n",
    "    return x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "8714158f97c7a3374982254e96b4fb0be2c5d67b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "register_tokenizer('space')(cysplit)\n",
    "register_tokenizer('word_tokenize')(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9667c26e4e72bf07c7f6ff42538574162ccbdadc"
   },
   "source": [
    "### Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "79f3c1b5119a8bdc16269671dcdbfd72ee44a21d"
   },
   "outputs": [],
   "source": [
    "class TextNormalizerWrapper(object):\n",
    "\n",
    "    registry = NORMALIZER_REGISTRY\n",
    "    default_config = None\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.normalizers = [self.registry[n] for n in config.normalizers]\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(cls, parser):\n",
    "        assert isinstance(cls.default_config, dict)\n",
    "        parser.add_argument(\n",
    "            '--normalizers', nargs='+', choices=cls.registry)\n",
    "        parser.set_defaults(**cls.default_config)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for normalizer in self.normalizers:\n",
    "            x = normalizer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class TextTokenizerWrapper(object):\n",
    "\n",
    "    registry = TOKENIZER_REGISTRY\n",
    "    default_config = None\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.tokenizer = self.registry[config.tokenizer]\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(cls, parser):\n",
    "        assert isinstance(cls.default_config, dict)\n",
    "        parser.add_argument('--tokenizer', choices=cls.registry)\n",
    "        parser.set_defaults(**cls.default_config)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.tokenizer(x)\n",
    "\n",
    "    \n",
    "class WordEmbeddingFeaturizerWrapper(object):\n",
    "\n",
    "    registry = WORD_EMBEDDING_FEATURIZER_REGISTRY\n",
    "    default_config = None\n",
    "    default_extra_config = None\n",
    "\n",
    "    def __init__(self, config, vocab):\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "        self.featurizers = {\n",
    "            k: self.registry[k](config, vocab)\n",
    "            for k in config.word_embedding_features}\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(cls, parser):\n",
    "        assert isinstance(cls.default_config, dict)\n",
    "        parser.add_argument(\n",
    "            '--use-pretrained-vectors', nargs='+',\n",
    "            choices=['glove', 'paragram', 'wnews', 'gnews'])\n",
    "        parser.add_argument(\n",
    "            '--word-embedding-features', nargs='+', choices=cls.registry)\n",
    "        parser.set_defaults(**cls.default_config)\n",
    "\n",
    "    @classmethod\n",
    "    def add_extra_args(cls, parser, config):\n",
    "        assert isinstance(cls.default_extra_config, dict)\n",
    "        for featurizer in config.word_embedding_features:\n",
    "            cls.registry[featurizer].add_args(parser)\n",
    "        parser.set_defaults(**cls.default_extra_config)\n",
    "\n",
    "    def __call__(self, features, datasets):\n",
    "        return {k: feat(features, datasets)\n",
    "                for k, feat in self.featurizers.items()}\n",
    "\n",
    "\n",
    "class WordExtraFeaturizerWrapper(object):\n",
    "\n",
    "    registry = WORD_EXTRA_FEATURIZER_REGISTRY\n",
    "    default_config = None\n",
    "\n",
    "    def __init__(self, config, vocab):\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "        self.featurizers = {\n",
    "            k: self.registry[k]() for k in config.word_extra_features}\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(cls, parser):\n",
    "        parser.add_argument(\n",
    "            '--word-extra-features', nargs='+', choices=cls.registry)\n",
    "        parser.set_defaults(**cls.default_config)\n",
    "\n",
    "    def __call__(self, vocab):\n",
    "        empty = np.empty([len(vocab), 0])\n",
    "        return np.concatenate([empty, *[\n",
    "            f(vocab) for f in self.featurizers.values()]], axis=1)\n",
    "\n",
    "\n",
    "class SentenceExtraFeaturizerWrapper(object):\n",
    "\n",
    "    registry = SENTENCE_EXTRA_FEATURIZER_REGISTRY\n",
    "    default_config = None\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.featurizers = {\n",
    "            k: self.registry[k]() for k in config.sentence_extra_features}\n",
    "        self.n_dims = sum(list(f.n_dims for f in self.featurizers.values()))\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(cls, parser):\n",
    "        parser.add_argument(\n",
    "            '--sentence-extra-features', nargs='+', choices=cls.registry)\n",
    "        parser.set_defaults(**cls.default_config)\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        empty = np.empty((0,))\n",
    "        return np.concatenate([empty, *[\n",
    "            f(sentence) for f in self.featurizers.values()]], axis=0)\n",
    "\n",
    "    def fit_standardize(self, features):\n",
    "        assert features.ndim == 2\n",
    "        self.mean = features.mean(axis=0)\n",
    "        self.std = features.std(axis=0)\n",
    "        self.std = np.where(self.std != 0, self.std, 1)\n",
    "        return (features - self.mean) / self.std\n",
    "\n",
    "    def standardize(self, features):\n",
    "        assert hasattr(self, 'mean'), hasattr(self, 'std')\n",
    "        return (features - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9aecfa261368ff7b834d4dfb5257cf26e3d3b877"
   },
   "source": [
    "### Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "5c90256ff3625e2f0e0c96eecf30e502ca9159b6"
   },
   "outputs": [],
   "source": [
    "class WordbasedPreprocessor():\n",
    "\n",
    "    def tokenize(self, datasets, normalizer, tokenizer):\n",
    "        tokenize = Pipeline(normalizer, tokenizer)\n",
    "        apply_tokenize = ApplyNdArray(tokenize, processes=2, dtype=object)\n",
    "        tokens = [apply_tokenize(d.df.question_text.values) for d in datasets]\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, datasets, config):\n",
    "        train_dataset, test_dataset, submit_dataset = datasets\n",
    "        vocab = WordVocab(mincount=config.vocab_mincount)\n",
    "        vocab.add_documents(train_dataset.positives.tokens, 'train-pos')\n",
    "        vocab.add_documents(train_dataset.negatives.tokens, 'train-neg')\n",
    "        vocab.add_documents(test_dataset.positives.tokens, 'test-pos')\n",
    "        vocab.add_documents(test_dataset.negatives.tokens, 'test-neg')\n",
    "        vocab.add_documents(submit_dataset.df.tokens, 'submit')\n",
    "        vocab.build()\n",
    "        return vocab\n",
    "\n",
    "    def build_tokenids(self, datasets, vocab, config):\n",
    "        token2id = lambda xs: pad_sequence(  # NOQA\n",
    "            [vocab.token2id[x] for x in xs], config.maxlen)\n",
    "        apply_token2id = ApplyNdArray(\n",
    "            token2id, processes=1, dtype='i', dims=(config.maxlen,))\n",
    "        tokenids = [apply_token2id(d.df.tokens.values) for d in datasets]\n",
    "        return tokenids\n",
    "\n",
    "    def build_sentence_features(self, datasets, sentence_extra_featurizer):\n",
    "        train_dataset, test_dataset, submit_dataset = datasets\n",
    "        apply_featurize = ApplyNdArray(\n",
    "            sentence_extra_featurizer, processes=1, dtype='f',\n",
    "            dims=(sentence_extra_featurizer.n_dims,))\n",
    "        _X2 = [apply_featurize(d.df.question_text.values) for d in datasets]\n",
    "        _train_X2, _test_X2, _submit_X2 = _X2\n",
    "        train_X2 = sentence_extra_featurizer.fit_standardize(_train_X2)\n",
    "        test_X2 = sentence_extra_featurizer.standardize(_test_X2)\n",
    "        submit_X2 = sentence_extra_featurizer.standardize(_submit_X2)\n",
    "        return train_X2, test_X2, submit_X2\n",
    "\n",
    "    def build_embedding_matrices(self, datasets, word_embedding_featurizer,\n",
    "                                 vocab, pretrained_vectors):\n",
    "        pretrained_vectors_merged = np.stack(\n",
    "            [wv.vectors for wv in pretrained_vectors.values()]).mean(axis=0)\n",
    "        vocab.unk = (pretrained_vectors_merged == 0).all(axis=1)\n",
    "        vocab.known = ~vocab.unk\n",
    "        embedding_matrices = word_embedding_featurizer(\n",
    "            pretrained_vectors_merged, datasets)\n",
    "        return embedding_matrices\n",
    "\n",
    "    def build_word_features(self, word_embedding_featurizer,\n",
    "                            embedding_matrices, word_extra_features):\n",
    "        embedding = np.stack(list(embedding_matrices.values()))\n",
    "        embedding = embedding.mean(axis=0)\n",
    "        word_features = np.concatenate(\n",
    "            [embedding, word_extra_features], axis=1)\n",
    "        return word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c3f4efda0306adace01c1f34cdf16e42345ca91"
   },
   "source": [
    "## NN modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b710c056eb82dc780fda757bf5425b0e7dcdf89"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "299eb88ddb5d16621fb9a8bea4bb3bc30a2fe431"
   },
   "outputs": [],
   "source": [
    "class RNNEncoderBase(nn.Module):\n",
    "\n",
    "    def __init__(self, config, modules, in_size):\n",
    "        super().__init__()\n",
    "        rnns = []\n",
    "        input_size = in_size\n",
    "        for module in modules:\n",
    "            rnn = module(\n",
    "                input_size=input_size,\n",
    "                hidden_size=config.encoder_n_hidden,\n",
    "                bidirectional=config.encoder_bidirectional,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            n_direction = int(config.encoder_bidirectional) + 1\n",
    "            input_size = n_direction * config.encoder_n_hidden\n",
    "            rnns.append(rnn)\n",
    "        self.rnns = nn.ModuleList(rnns)\n",
    "        self.out_size = n_direction * config.encoder_n_hidden\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(self, parser):\n",
    "        parser.add_argument('--encoder-bidirectional', type=bool, default=True)\n",
    "        parser.add_argument('--encoder-dropout', type=float, default=0.)\n",
    "        parser.add_argument('--encoder-n-hidden', type=int)\n",
    "        parser.add_argument('--encoder-n-layers', type=int)\n",
    "        parser.add_argument('--encoder-aggregator', type=str,\n",
    "                            choices=AGGREGATOR_REGISTRY)\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        h = input\n",
    "        for rnn in self.rnns:\n",
    "            h, _ = rnn(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "@register_encoder('lstm')\n",
    "class LSTMEncoder(RNNEncoderBase):\n",
    "\n",
    "    def __init__(self, config, in_size):\n",
    "        modules = [nn.LSTM] * config.encoder_n_layers\n",
    "        super().__init__(config, modules, in_size)\n",
    "\n",
    "\n",
    "@register_encoder('gru')\n",
    "class GRUEncoder(RNNEncoderBase):\n",
    "\n",
    "    def __init__(self, config, in_size):\n",
    "        assert config.encoder_n_layers > 1\n",
    "        modules = [nn.GRU] * config.encoder_n_layers\n",
    "        super().__init__(config, modules, in_size)\n",
    "\n",
    "\n",
    "@register_encoder('lstmgru')\n",
    "class LSTMGRUEncoder(RNNEncoderBase):\n",
    "\n",
    "    def __init__(self, config, in_size):\n",
    "        assert config.encoder_n_layers > 1\n",
    "        modules = [nn.LSTM] * (config.encoder_n_layers - 1) + [nn.GRU]\n",
    "        super().__init__(config, modules, in_size)\n",
    "\n",
    "\n",
    "@register_encoder('grulstm')\n",
    "class GRULSTMEncoder(RNNEncoderBase):\n",
    "\n",
    "    def __init__(self, config, in_size):\n",
    "        assert config.encoder_n_layers > 1\n",
    "        modules = [nn.GRU] * (config.encoder_n_layers - 1) + [nn.LSTM]\n",
    "        super().__init__(config, modules, in_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0d2ca402d44ddadf492f4acc1ecc6a271b4653b"
   },
   "source": [
    "### Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "8e2807bdbc3838e10b549a6188f0096a5f5bcdf1"
   },
   "outputs": [],
   "source": [
    "@register_aggregator('max')\n",
    "class MaxPoolingAggregator(nn.Module):\n",
    "\n",
    "    def __call__(self, hs, mask):\n",
    "        if mask is not None:\n",
    "            hs = hs.masked_fill(~mask.unsqueeze(2), -np.inf)\n",
    "        h = hs.max(dim=1)[0]\n",
    "        return h\n",
    "\n",
    "\n",
    "@register_aggregator('sum')\n",
    "class SumPoolingAggregator(nn.Module):\n",
    "\n",
    "    def __call__(self, hs, mask):\n",
    "        if mask is not None:\n",
    "            hs = hs.masked_fill(~mask.unsqueeze(2), 0)\n",
    "        h = hs.sum(dim=1)\n",
    "        return h\n",
    "\n",
    "\n",
    "@register_aggregator('avg')\n",
    "class AvgPoolingAggregator(nn.Module):\n",
    "\n",
    "    def __call__(self, hs, mask):\n",
    "        if mask is not None:\n",
    "            hs = hs.masked_fill(~mask.unsqueeze(2), 0)\n",
    "        h = hs.sum(dim=1)\n",
    "        maxlen = mask.sum(dim=1)\n",
    "        h /= maxlen[:, None].type(torch.float)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "499506a2b873dcdc3fb5f3066cf5e6b7783d6ced"
   },
   "source": [
    "### Ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "5d94c80036184d23bcc0b5f72c541299af7760cc"
   },
   "outputs": [],
   "source": [
    "class BaseEnsembler(metaclass=ABCMeta):\n",
    "\n",
    "    def __init__(self, config, models, results):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.models = models\n",
    "        self.results = results\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X, t, test_size=0.1):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_proba(self, X, X2):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X, X2):\n",
    "        y = self.predict_proba(X, X2)\n",
    "        return (y > self.threshold).astype('i')\n",
    "\n",
    "    \n",
    "class AverageEnsembler(BaseEnsembler):\n",
    "\n",
    "    def __init__(self, config, models, results):\n",
    "        self.config = config\n",
    "        self.models = models\n",
    "        self.results = results\n",
    "        self.device = config.device\n",
    "        self.batchsize_train = config.batchsize\n",
    "        self.batchsize_valid = config.batchsize_valid\n",
    "        self.threshold_cv = np.array(\n",
    "            [m.threshold for m in models]).mean()\n",
    "        self.threshold = self.threshold_cv\n",
    "\n",
    "    def fit(self, X, X2, t, test_size=0.1):\n",
    "        # Nothing to do\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, X, X2):\n",
    "        pred_X = X.to(self.device)\n",
    "        pred_X2 = X2.to(self.device)\n",
    "        dataset = torch.utils.data.TensorDataset(pred_X, pred_X2)\n",
    "        iterator = DataLoader(\n",
    "            dataset, batch_size=self.batchsize_valid, shuffle=False)\n",
    "        ys = defaultdict(list)\n",
    "        for batch in tqdm(iterator, desc='submit', leave=False):\n",
    "            for i, model in enumerate(self.models):\n",
    "                model.eval()\n",
    "                ys[i].append(model.predict_proba(*batch))\n",
    "        ys = np.concatenate(\n",
    "            [np.concatenate(_ys) for _ys in ys.values()], axis=1)\n",
    "        y = ys.mean(axis=1, keepdims=True)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c99afd2aa368d75d5ce34fc4baac133ceae654e1"
   },
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7b424873f73da2526c875dd4155fa0ed579a6ef7"
   },
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "\n",
    "    default_config = None\n",
    "\n",
    "    def __init__(self, embedding, encoder, aggregator, mlp, out, lossfunc):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.encoder = encoder\n",
    "        self.aggregator = aggregator\n",
    "        self.mlp = mlp\n",
    "        self.out = out\n",
    "        self.lossfunc = lossfunc\n",
    "\n",
    "    def calc_loss(self, X, X2, t, W=None):\n",
    "        y = self.forward(X, X2)\n",
    "        loss = self.lossfunc(y, t)\n",
    "        output = dict(\n",
    "            y=torch.sigmoid(y).cpu().detach().numpy(),\n",
    "            t=t.cpu().detach().numpy(),\n",
    "            loss=loss.cpu().detach().numpy(),\n",
    "        )\n",
    "        return loss, output\n",
    "\n",
    "    def to_device(self, device):\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        return self\n",
    "\n",
    "    def forward(self, X, X2):\n",
    "        h = self.predict_features(X, X2)\n",
    "        out = self.out(h)\n",
    "        return out\n",
    "\n",
    "    def predict_proba(self, X, X2):\n",
    "        y = self.forward(X, X2)\n",
    "        proba = torch.sigmoid(y).cpu().detach().numpy()\n",
    "        return proba\n",
    "\n",
    "    def predict_features(self, X, X2):\n",
    "        mask = X != 0\n",
    "        maxlen = (mask == 1).any(dim=0).sum()\n",
    "        X = X[:, :maxlen]\n",
    "        mask = mask[:, :maxlen]\n",
    "\n",
    "        h = self.embedding(X)\n",
    "        h = self.encoder(h, mask)\n",
    "        h = self.aggregator(h, mask)\n",
    "        h = self.mlp(h, X2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2dcb12487daf529b254a65336b5b9a1d866857ac"
   },
   "source": [
    "### Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "8ec05bb6aacf562834ac6ba0ec7b436f54b56550"
   },
   "outputs": [],
   "source": [
    "class NNModuleWrapperBase(nn.Module, metaclass=ABCMeta):\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_args(cls, parser):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_extra_args(cls, parser):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "class EmbeddingWrapper(NNModuleWrapperBase):\n",
    "\n",
    "    default_config = None\n",
    "\n",
    "    def __init__(self, config, embedding_matrix):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.module = nn.Embedding.from_pretrained(\n",
    "            torch.Tensor(embedding_matrix), freeze=True)\n",
    "        if self.config.embedding_dropout1d > 0:\n",
    "            self.dropout1d = nn.Dropout(config.embedding_dropout1d)\n",
    "        if self.config.embedding_dropout2d > 0:\n",
    "            self.dropout2d = nn.Dropout2d(config.embedding_dropout2d)\n",
    "        if self.config.embedding_spatial_dropout > 0:\n",
    "            self.spatial_dropout = nn.Dropout2d(\n",
    "                config.embedding_spatial_dropout)\n",
    "        self.out_size = embedding_matrix.shape[1]\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(cls, parser):\n",
    "        assert isinstance(cls.default_config, dict)\n",
    "        parser.add_argument('--embedding-dropout1d', type=float, default=0.)\n",
    "        parser.add_argument('--embedding-dropout2d', type=float, default=0.)\n",
    "        parser.add_argument('--embedding-spatial-dropout',\n",
    "                            type=float, default=0.)\n",
    "        parser.set_defaults(**cls.default_config)\n",
    "\n",
    "    @classmethod\n",
    "    def add_extra_args(cls, parser, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.module(X)\n",
    "        if self.config.embedding_dropout1d > 0:\n",
    "            h = self.dropout1d(h)\n",
    "        if self.config.embedding_dropout2d > 0:\n",
    "            h = self.dropout2d(h)\n",
    "        if self.config.embedding_spatial_dropout > 0:\n",
    "            h = h.permute(0, 2, 1)\n",
    "            h = self.spatial_dropout(h)\n",
    "            h = h.permute(0, 2, 1)\n",
    "        return h\n",
    "\n",
    "    \n",
    "class EncoderWrapper(nn.Module):\n",
    "\n",
    "    registry = ENCODER_REGISTRY\n",
    "\n",
    "    def __init__(self, config, in_size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.module = self.registry[config.encoder](config, in_size)\n",
    "        self.out_size = self.module.out_size\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(cls, parser):\n",
    "        assert isinstance(cls.default_config, dict)\n",
    "        parser.add_argument(\n",
    "            '--encoder', choices=cls.registry)\n",
    "        parser.set_defaults(**cls.default_config)\n",
    "\n",
    "    @classmethod\n",
    "    def add_extra_args(cls, parser, config):\n",
    "        assert isinstance(cls.default_extra_config, dict)\n",
    "        cls.registry[config.encoder].add_args(parser)\n",
    "        parser.set_defaults(**cls.default_extra_config)\n",
    "\n",
    "    def forward(self, X, mask):\n",
    "        h = self.module(X, mask)\n",
    "        return h\n",
    "\n",
    "    \n",
    "class AggregatorWrapper(NNModuleWrapperBase):\n",
    "\n",
    "    default_config = None\n",
    "    registry = AGGREGATOR_REGISTRY\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.module = self.registry[config.aggregator]()\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(cls, parser):\n",
    "        assert isinstance(cls.default_config, dict)\n",
    "        parser.add_argument('--aggregator',\n",
    "                            choices=cls.registry)\n",
    "        parser.set_defaults(**cls.default_config)\n",
    "\n",
    "    @classmethod\n",
    "    def add_extra_args(cls, parser, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X, mask):\n",
    "        h = self.module(X, mask)\n",
    "        return h\n",
    "\n",
    "    \n",
    "class MLPWrapper(NNModuleWrapperBase):\n",
    "\n",
    "    default_config = None\n",
    "\n",
    "    def __init__(self, config, in_size):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.config = config\n",
    "        assert isinstance(config.mlp_n_hiddens, list)\n",
    "        layers = []\n",
    "        if config.mlp_bn0:\n",
    "            layers.append(nn.BatchNorm1d(in_size))\n",
    "        if config.mlp_dropout0 > 0:\n",
    "            layers.append(nn.Dropout(config.mlp_dropout0))\n",
    "        for n_hidden in config.mlp_n_hiddens:\n",
    "            layers.append(nn.Linear(in_size, n_hidden))\n",
    "            if config.mlp_actfun is not None:\n",
    "                layers.append(config.mlp_actfun)\n",
    "            if config.mlp_bn:\n",
    "                layers.append(nn.BatchNorm1d(n_hidden))\n",
    "            if config.mlp_dropout > 0:\n",
    "                layers.append(nn.Dropout(config.mlp_dropout))\n",
    "            in_size = n_hidden\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    @classmethod\n",
    "    def add_args(cls, parser):\n",
    "        assert isinstance(cls.default_config, dict)\n",
    "        parser.add_argument('--mlp-n-hiddens', type=list)\n",
    "        parser.add_argument('--mlp-bn', type=bool)\n",
    "        parser.add_argument('--mlp-bn0', type=bool)\n",
    "        parser.add_argument('--mlp-dropout', type=float, default=0.)\n",
    "        parser.add_argument('--mlp-dropout0', type=float, default=0.)\n",
    "        parser.add_argument('--mlp-actfun', default=0.)\n",
    "        parser.set_defaults(**cls.default_config)\n",
    "\n",
    "    @classmethod\n",
    "    def add_extra_args(cls, parser, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X, X2):\n",
    "        h = X\n",
    "        if X.shape[1] + X2.shape[1] == self.in_size:\n",
    "            h = torch.cat([h, X2], dim=1)\n",
    "        h = self.layers(h)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7570cb3f9c60ca8e79132669f14a4045276176dc"
   },
   "source": [
    "## Presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "9473fb9029e9a68c8dfa865a0674e0c0bfaf74e1"
   },
   "outputs": [],
   "source": [
    "# =======  Experiment configuration  =======\n",
    "\n",
    "class ExperimentConfigBuilderPresets(ExperimentConfigBuilderBase):\n",
    "\n",
    "    default_config = dict(\n",
    "        maxlen=72,\n",
    "        vocab_mincount=5,\n",
    "        scale_batchsize=[],\n",
    "        validate_from=2,\n",
    "    )\n",
    "\n",
    "\n",
    "# =======  Preprocessing modules  =======\n",
    "\n",
    "class TextNormalizerPresets(TextNormalizerWrapper):\n",
    "\n",
    "    default_config = dict(\n",
    "        normalizers=[\n",
    "            'lower',\n",
    "            'misspell',\n",
    "            'punct',\n",
    "            'number+underscore'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "class TextTokenizerPresets(TextTokenizerWrapper):\n",
    "\n",
    "    default_config = dict(\n",
    "        tokenizer='space'\n",
    "    )\n",
    "\n",
    "\n",
    "class WordEmbeddingFeaturizerPresets(WordEmbeddingFeaturizerWrapper):\n",
    "\n",
    "    default_config = dict(\n",
    "        use_pretrained_vectors=['glove', 'paragram'],\n",
    "        word_embedding_features=['pretrained', 'word2vec'],\n",
    "    )\n",
    "    default_extra_config = dict(\n",
    "        finetune_word2vec_init_unk='zeros',\n",
    "        finetune_word2vec_mincount=1,\n",
    "        finetune_word2vec_workers=1,\n",
    "        finetune_word2vec_iter=5,\n",
    "        finetune_word2vec_size=300,\n",
    "        finetune_word2vec_sg=0,\n",
    "        finetune_word2vec_sorted_vocab=0,\n",
    "    )\n",
    "\n",
    "\n",
    "class WordExtraFeaturizerPresets(WordExtraFeaturizerWrapper):\n",
    "\n",
    "    default_config = dict(\n",
    "        word_extra_features=[],\n",
    "    )\n",
    "\n",
    "\n",
    "class SentenceExtraFeaturizerPresets(SentenceExtraFeaturizerWrapper):\n",
    "\n",
    "    default_config = dict(\n",
    "        sentence_extra_features=[],\n",
    "    )\n",
    "\n",
    "\n",
    "class PreprocessorPresets(WordbasedPreprocessor):\n",
    "\n",
    "    def build_word_features(self, word_embedding_featurizer,\n",
    "                            embedding_matrices, word_extra_features):\n",
    "        embedding = np.stack(list(embedding_matrices.values()))\n",
    "\n",
    "        # Add noise\n",
    "        unk = (embedding[0] == 0).all(axis=1)\n",
    "        mean, std = embedding[0, ~unk].mean(), embedding[0, ~unk].std()\n",
    "        unk_and_hfq = unk & word_embedding_featurizer.vocab.hfq\n",
    "        noise = np.random.normal(\n",
    "            mean, std, (unk_and_hfq.sum(), embedding[0].shape[1]))\n",
    "        embedding[0, unk_and_hfq] = noise\n",
    "        embedding[0, 0] = 0\n",
    "\n",
    "        embedding = embedding.mean(axis=0)\n",
    "        word_features = np.concatenate(\n",
    "            [embedding, word_extra_features], axis=1)\n",
    "        return word_features\n",
    "\n",
    "\n",
    "# =======  Training modules  =======\n",
    "\n",
    "class EmbeddingPresets(EmbeddingWrapper):\n",
    "\n",
    "    default_config = dict(\n",
    "        embedding_dropout1d=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "class EncoderPresets(EncoderWrapper):\n",
    "\n",
    "    default_config = dict(\n",
    "        encoder='lstm',\n",
    "    )\n",
    "    default_extra_config = dict(\n",
    "        encoder_bidirectional=True,\n",
    "        encoder_dropout=0.,\n",
    "        encoder_n_layers=2,\n",
    "        encoder_n_hidden=128,\n",
    "    )\n",
    "\n",
    "\n",
    "class AggregatorPresets(AggregatorWrapper):\n",
    "\n",
    "    default_config = dict(\n",
    "        aggregator='max',\n",
    "    )\n",
    "\n",
    "\n",
    "class MLPPresets(MLPWrapper):\n",
    "\n",
    "    default_config = dict(\n",
    "        mlp_n_hiddens=[128, 128],\n",
    "        mlp_bn0=False,\n",
    "        mlp_dropout0=0.,\n",
    "        mlp_bn=True,\n",
    "        mlp_actfun=nn.ReLU(True),\n",
    "    )\n",
    "\n",
    "\n",
    "class EnsemblerPresets(AverageEnsembler):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a50590eb55f7652b81ca1548edc46e468cc52201"
   },
   "source": [
    "## Training modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8d4d183555c69f4b60266cacc37721740b9e76b"
   },
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "6402a4ca7280ac6174c390f94605ae8b106fc74d"
   },
   "outputs": [],
   "source": [
    "def classification_metrics(ys, ts):\n",
    "    scores = {}\n",
    "\n",
    "    if len(np.unique(ts)) > 1:\n",
    "        # Search optimal threshold\n",
    "        precs, recs, thresholds = metrics.precision_recall_curve(ts, ys)\n",
    "        thresholds = np.append(thresholds, 1.001)\n",
    "        idx = (precs != 0) * (recs != 0)\n",
    "        precs, recs, thresholds = precs[idx], recs[idx], thresholds[idx]\n",
    "        fbetas = 2 / (1 / precs + 1 / recs)\n",
    "        best_idx = np.argmax(fbetas)\n",
    "        threshold = thresholds[best_idx]\n",
    "        prec = precs[best_idx]\n",
    "        rec = recs[best_idx]\n",
    "        fbeta = fbetas[best_idx]\n",
    "\n",
    "        scores['ap'] = metrics.average_precision_score(ts, ys)\n",
    "        scores['rocauc'] = metrics.roc_auc_score(ts, ys)\n",
    "        scores['threshold'] = threshold\n",
    "        scores['prec'] = prec\n",
    "        scores['rec'] = rec\n",
    "        scores['fbeta'] = fbeta\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "class ClassificationResult(object):\n",
    "\n",
    "    def __init__(self, name, outdir=None, postfix=None, main_metrics='fbeta'):\n",
    "        self.initialize()\n",
    "        self.name = name\n",
    "        self.postfix = postfix\n",
    "        self.outdir = outdir\n",
    "        self.summary = None\n",
    "        self.main_metrics = main_metrics\n",
    "        self.n_trained = 0\n",
    "\n",
    "    def initialize(self):\n",
    "        self.losses = []\n",
    "        self.ys = []\n",
    "        self.ts = []\n",
    "\n",
    "    def add_record(self, loss, y, t):\n",
    "        self.losses.append(loss)\n",
    "        self.ys.append(y)\n",
    "        self.ts.append(t)\n",
    "        self.n_trained += len(y)\n",
    "\n",
    "    def calc_score(self, epoch):\n",
    "        loss = np.array(self.losses).mean()\n",
    "        self.ys, self.ts = np.concatenate(self.ys), np.concatenate(self.ts)\n",
    "        score = classification_metrics(self.ys, self.ts)\n",
    "        summary = dict(name=self.name, loss=loss, **score)\n",
    "        if len(score) > 0:\n",
    "            if self.summary is None:\n",
    "                self.summary = pd.DataFrame([summary], index=[epoch])\n",
    "                self.summary.index.name = 'epoch'\n",
    "            else:\n",
    "                self.summary.loc[epoch] = summary\n",
    "        if self.best_epoch == epoch:\n",
    "            self.best_ys = self.ys\n",
    "            self.best_ts = self.ts\n",
    "        self.initialize()\n",
    "\n",
    "    def get_dict(self):\n",
    "        loss, fbeta, epoch = 0, 0, 0\n",
    "        if self.summary is not None:\n",
    "            row = self.summary.iloc[-1]\n",
    "            epoch = row.name\n",
    "            loss = row.loss\n",
    "            fbeta = row.fbeta\n",
    "        return {\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'fbeta': fbeta,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def fbeta(self):\n",
    "        if self.summary is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.summary.fbeta[-1]\n",
    "\n",
    "    @property\n",
    "    def best_fbeta(self):\n",
    "        return self.summary[self.main_metrics].max()\n",
    "\n",
    "    @property\n",
    "    def best_epoch(self):\n",
    "        return self.summary[self.main_metrics].idxmax()\n",
    "\n",
    "    @property\n",
    "    def best_threshold(self):\n",
    "        idx = self.summary[self.main_metrics].idxmax()\n",
    "        return self.summary['threshold'][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e30da7b12dc882bcb1fb917bc7b1fe3bce43f315"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "bceeedb79f91b2136fb6a0bfe077e9cbe05c166f"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "cdef class ApplyNdArray:\n",
    "    cdef func\n",
    "    cdef dtype\n",
    "    cdef dims\n",
    "    cdef int processes\n",
    "\n",
    "    def __init__(self, func, processes=1, dtype=object, dims=None):\n",
    "        self.func = func\n",
    "        self.processes = processes\n",
    "        self.dtype = dtype\n",
    "        self.dims = dims\n",
    "\n",
    "    def __call__(self, arr):\n",
    "        if self.processes == 1:\n",
    "            return self.apply(arr)\n",
    "        else:\n",
    "            return self.apply_parallel(arr)\n",
    "\n",
    "    cpdef apply(self, arr):\n",
    "        cdef int i\n",
    "        cdef int n = len(arr)\n",
    "        if self.dims is not None:\n",
    "            shape = (n, *self.dims)\n",
    "        else:\n",
    "            shape = n\n",
    "        cdef res = np.empty(shape, dtype=self.dtype)\n",
    "        for i in range(n):\n",
    "            res[i] = self.func(arr[i])\n",
    "        return res\n",
    "\n",
    "    cpdef apply_parallel(self, arr):\n",
    "        cdef list arrs = np.array_split(arr, self.processes)\n",
    "        with Pool(processes=self.processes) as pool:\n",
    "            outputs = pool.map(self.apply, arrs)\n",
    "        return np.concatenate(outputs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "37299fdada70ec09b6275849662dc47c9bc2e5d2"
   },
   "outputs": [],
   "source": [
    "def load_module(filename):\n",
    "    assert isinstance(filename, Path)\n",
    "    name = filename.stem\n",
    "    spec = importlib.util.spec_from_file_location(name, filename)\n",
    "    mod = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(mod)\n",
    "    sys.modules[mod.__name__] = mod\n",
    "    return mod\n",
    "\n",
    "\n",
    "def rmtree_after_confirmation(path, force=False):\n",
    "    if Path(path).exists():\n",
    "        if not force and not prompter.yesno('Overwrite %s?' % path):\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "\n",
    "def pad_sequence(xs, length, padding_value=0):\n",
    "    assert isinstance(xs, list)\n",
    "    n_padding = length - len(xs)\n",
    "    return np.array(xs + [padding_value] * n_padding, 'i')[:length]\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class Pipeline(object):\n",
    "\n",
    "    def __init__(self, *modules):\n",
    "        self.modules = modules\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for module in self.modules:\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "24fe4916af2f186063ad06d34493e6f6963d8c32"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6eae683c4269ba9340161b512c78a28c6b205057"
   },
   "source": [
    "## Setup & preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "cdc1274a8e6aa310796cf37b98a6cabdddf23862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(aggregator='max', batchsize=512, batchsize_valid=1024, cv=5, cv_part=None, device=0, embedding_dropout1d=0.2, embedding_dropout2d=0.0, embedding_spatial_dropout=0.0, encoder='lstm', encoder_aggregator=None, encoder_bidirectional=True, encoder_dropout=0.0, encoder_n_hidden=128, encoder_n_layers=2, ensembler_n_snapshots=1, epochs=5, finetune_word2vec_init_unk='zeros', finetune_word2vec_iter=5, finetune_word2vec_mincount=1, finetune_word2vec_sg=0, finetune_word2vec_size=300, finetune_word2vec_sorted_vocab=0, finetune_word2vec_window=5, finetune_word2vec_workers=1, gridsearch=False, holdout=False, logging=False, lr=0.001, maxlen=72, mlp_actfun=ReLU(inplace), mlp_bn=True, mlp_bn0=False, mlp_dropout=0.0, mlp_dropout0=0.0, mlp_n_hiddens=[128, 128], modelfile=None, n_rows=None, normalizers=['lower', 'misspell', 'punct', 'number+underscore'], optuna_trials=None, outdir=PosixPath('.'), outdir_bottom='default', outdir_top=PosixPath('results'), pos_weight=1.0, processes=2, scale_batchsize=[], seed=1029, sentence_extra_features=['char', 'word'], test=False, tokenizer='space', use_pretrained_vectors=['glove', 'paragram'], validate_from=4, vocab_mincount=5, word_embedding_features=['pretrained', 'word2vec'], word_extra_features=['idf', 'unk'])\n",
      "CPU times: user 3.86 s, sys: 340 ms, total: 4.2 s\n",
      "Wall time: 4.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "exec(modules)\n",
    "config = ExperimentConfigBuilder().build(args=[])\n",
    "print(config)\n",
    "start = time.time()\n",
    "set_seed(config.seed)\n",
    "\n",
    "train_df, submit_df = load_qiqc(n_rows=config.n_rows)\n",
    "datasets = build_datasets(train_df, submit_df, config.holdout, config.seed)\n",
    "train_dataset, test_dataset, submit_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "24b919658c124fa870172c427af2984ce2d3a4ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize texts...\n",
      "CPU times: user 7.22 s, sys: 1.99 s, total: 9.2 s\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Tokenize texts...')\n",
    "preprocessor = Preprocessor()\n",
    "normalizer = TextNormalizer(config)\n",
    "tokenizer = TextTokenizer(config)\n",
    "train_dataset.tokens, test_dataset.tokens, submit_dataset.tokens = \\\n",
    "    preprocessor.tokenize(datasets, normalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "4181f91134b4f874e676b9e1f5b2c4d30567a54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build vocabulary...\n",
      "CPU times: user 31.8 s, sys: 352 ms, total: 32.1 s\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Build vocabulary...')\n",
    "vocab = preprocessor.build_vocab(datasets, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c63de35321d1c8b00bf0d6115c21b0200d1b2c19"
   },
   "source": [
    "## Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "255daafa472ee422e12d1bff536ad118f97910cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build token ids...\n",
      "CPU times: user 20.5 s, sys: 324 ms, total: 20.8 s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Build token ids...')\n",
    "train_dataset.tids, test_dataset.tids, submit_dataset.tids = \\\n",
    "    preprocessor.build_tokenids(datasets, vocab, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ebeb2420fba705a8177f21b8b65e9a02e8ee0e5"
   },
   "source": [
    "## Build featurizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "88b1726354e1487ccdc43cedb028f147ee5c74b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build sentence extra features...\n",
      "CPU times: user 30.2 s, sys: 1.4 s, total: 31.6 s\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Build sentence extra features...')\n",
    "sentence_extra_featurizer = SentenceExtraFeaturizer(config)\n",
    "train_dataset._X2, test_dataset._X2, submit_dataset._X2 = \\\n",
    "    preprocessor.build_sentence_features(\n",
    "        datasets, sentence_extra_featurizer)\n",
    "[d.build(config.device) for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "6de736b25070158d393062b2113001176a8566d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained vectors...\n",
      "CPU times: user 2.44 s, sys: 2.74 s, total: 5.18 s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Load pretrained vectors...')\n",
    "pretrained_vectors = load_pretrained_vectors(\n",
    "    config.use_pretrained_vectors, vocab.token2id, test=config.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "1071d85ca9f3b51de8e156f0a3163e8ee0c64b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build word embedding matrix...\n",
      "CPU times: user 5min 24s, sys: 2.77 s, total: 5min 27s\n",
      "Wall time: 5min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Build word embedding matrix...')\n",
    "word_embedding_featurizer = WordEmbeddingFeaturizer(config, vocab)\n",
    "embedding_matrices = preprocessor.build_embedding_matrices(\n",
    "    datasets, word_embedding_featurizer, vocab, pretrained_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "f366043c9adce2f305bb1927a8282a56d5897c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build word extra features...\n",
      "CPU times: user 24 ms, sys: 0 ns, total: 24 ms\n",
      "Wall time: 24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Build word extra features...')\n",
    "word_extra_featurizer = WordExtraFeaturizer(config, vocab)\n",
    "word_extra_features = word_extra_featurizer(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "edbe848c81ab4d19f3cc1cf17d71dc1192eb1dd6"
   },
   "source": [
    "## Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "ba7f1f85e7fdc25a9157e1316f5cf3364dc6972f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build models...\n",
      "CPU times: user 14.1 s, sys: 7.7 s, total: 21.8 s\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Build models...')\n",
    "word_features_cv = [\n",
    "    preprocessor.build_word_features(\n",
    "        word_embedding_featurizer, embedding_matrices, word_extra_features)\n",
    "    for i in range(config.cv)]\n",
    "\n",
    "models = [\n",
    "    build_model(\n",
    "        config, word_features, sentence_extra_featurizer.n_dims\n",
    "    ) for word_features in word_features_cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "fa4f5241149964fbf40ff06ba94354d4a27d2b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassifier(\n",
      "  (embedding): Embedding(\n",
      "    (module): Embedding(212418, 402)\n",
      "    (dropout1d): Dropout(p=0.2)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (module): LSTMEncoder(\n",
      "      (rnns): ModuleList(\n",
      "        (0): LSTM(402, 128, batch_first=True, bidirectional=True)\n",
      "        (1): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (aggregator): Aggregator(\n",
      "    (module): MaxPoolingAggregator()\n",
      "  )\n",
      "  (mlp): MLP(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=262, out_features=128, bias=True)\n",
      "      (1): ReLU(inplace)\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (4): ReLU(inplace)\n",
      "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (lossfunc): BCEWithLogitsLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "069f2f284733d64b44c106854e6df0e1637418b1"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "47e16610dab4b6adfa01e39cf5b179c4bdedea78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 1/2040 [00:00<03:50,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#---- cv: 0 / 5, epoch 0, time: 250.11730074882507\n",
      "             ap     fbeta      loss    ...           rec    rocauc  threshold\n",
      "name                                   ...                                   \n",
      "train  0.596646  0.590578  0.128037    ...      0.574943  0.943649   0.458763\n",
      "\n",
      "[1 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 1/2040 [00:00<03:48,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#---- cv: 0 / 5, epoch 1, time: 249.90297031402588\n",
      "             ap    fbeta      loss    ...           rec    rocauc  threshold\n",
      "name                                  ...                                   \n",
      "train  0.709571  0.68063  0.097731    ...      0.739561  0.967645   0.317466\n",
      "\n",
      "[1 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 1/2040 [00:00<03:46,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#---- cv: 0 / 5, epoch 2, time: 250.82224369049072\n",
      "             ap     fbeta      loss    ...           rec    rocauc  threshold\n",
      "name                                   ...                                   \n",
      "train  0.736355  0.699558  0.091723    ...      0.753002  0.972544   0.333545\n",
      "\n",
      "[1 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  61%|██████    | 1244/2040 [02:32<01:36,  8.23it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Start training...')\n",
    "splitter = sklearn.model_selection.StratifiedKFold(\n",
    "    n_splits=config.cv, shuffle=True, random_state=config.seed)\n",
    "train_results, valid_results = [], []\n",
    "best_models = []\n",
    "\n",
    "for i_cv, (train_indices, valid_indices) in enumerate(\n",
    "        splitter.split(train_dataset.df, train_dataset.df.target)):\n",
    "    if config.cv_part is not None and i_cv >= config.cv_part:\n",
    "        break\n",
    "    train_tensor = train_dataset.build_labeled_dataset(train_indices)\n",
    "    valid_tensor = train_dataset.build_labeled_dataset(valid_indices)\n",
    "    valid_iter = DataLoader(\n",
    "        valid_tensor, batch_size=config.batchsize_valid)\n",
    "\n",
    "    model = models.pop(0)\n",
    "    model = model.to_device(config.device)\n",
    "    model_snapshots = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), config.lr)\n",
    "    train_result = ClassificationResult('train', config.outdir, str(i_cv))\n",
    "    valid_result = ClassificationResult('valid', config.outdir, str(i_cv))\n",
    "\n",
    "    batchsize = config.batchsize\n",
    "    for epoch in range(config.epochs):\n",
    "        if epoch in config.scale_batchsize:\n",
    "            batchsize *= 2\n",
    "            print(f'Batchsize: {batchsize}')\n",
    "        epoch_start = time.time()\n",
    "        sampler = None\n",
    "        train_iter = DataLoader(\n",
    "            train_tensor, sampler=sampler, drop_last=True,\n",
    "            batch_size=batchsize, shuffle=sampler is None)\n",
    "        _summary = []\n",
    "\n",
    "        # Training loop\n",
    "        for i, batch in enumerate(\n",
    "                tqdm(train_iter, desc='train', leave=False)):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss, output = model.calc_loss(*batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_result.add_record(**output)\n",
    "        train_result.calc_score(epoch)\n",
    "        _summary.append(train_result.summary.iloc[-1])\n",
    "\n",
    "        # Validation loop\n",
    "        if epoch >= config.validate_from:\n",
    "            for i, batch in enumerate(\n",
    "                    tqdm(valid_iter, desc='valid', leave=False)):\n",
    "                model.eval()\n",
    "                loss, output = model.calc_loss(*batch)\n",
    "                valid_result.add_record(**output)\n",
    "            valid_result.calc_score(epoch)\n",
    "            _summary.append(valid_result.summary.iloc[-1])\n",
    "\n",
    "            _model = deepcopy(model)\n",
    "            _model.threshold = valid_result.summary.threshold[epoch]\n",
    "            model_snapshots.append(_model)\n",
    "\n",
    "        summary = pd.DataFrame(_summary).set_index('name')\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        pbar = '#' * (i_cv + 1) + '-' * (config.cv - 1 - i_cv)\n",
    "        tqdm.write(f'\\n{pbar} cv: {i_cv} / {config.cv}, epoch {epoch}, '\n",
    "                   f'time: {epoch_time}')\n",
    "        tqdm.write(str(summary))\n",
    "\n",
    "    train_results.append(train_result)\n",
    "    valid_results.append(valid_result)\n",
    "    best_indices = valid_result.summary.fbeta.argsort()[::-1]\n",
    "    best_models.extend([model_snapshots[i] for i in\n",
    "                        best_indices[:config.ensembler_n_snapshots]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fac52039ffe472b4259a4e38d14f73d43553e172"
   },
   "source": [
    "## Build ensembler and make CV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "2a701bb825b115be4207cddf9317f166d6283265"
   },
   "outputs": [],
   "source": [
    "# Build ensembler\n",
    "train_X, train_X2, train_t = \\\n",
    "    train_dataset.X, train_dataset.X2, train_dataset.t\n",
    "ensembler = Ensembler(config, best_models, valid_results)\n",
    "ensembler.fit(train_X, train_X2, train_t)\n",
    "scores = dict(\n",
    "    valid_fbeta=np.array([r.best_fbeta for r in valid_results]).mean(),\n",
    "    valid_epoch=np.array([r.best_epoch for r in valid_results]).mean(),\n",
    "    threshold_cv=ensembler.threshold_cv,\n",
    "    threshold=ensembler.threshold,\n",
    "    elapsed_time=time.time() - start,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f941f2b851391622cb51f4efeb9b5a88dcbe9435"
   },
   "source": [
    "## Holdout evaluation / Logging for error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "484bd320bc5899dccef95a0e493db5500f9246aa"
   },
   "outputs": [],
   "source": [
    "if config.holdout:\n",
    "    test_X, test_X2, test_t = \\\n",
    "        test_dataset.X, test_dataset.X2, test_dataset._t\n",
    "    y, t = ensembler.predict_proba(test_X, test_X2), test_t\n",
    "    y_pred = y > ensembler.threshold\n",
    "    y_pred_cv = y > ensembler.threshold_cv\n",
    "    result = classification_metrics(y_pred, t)\n",
    "    result_cv = classification_metrics(y_pred_cv, t)\n",
    "    result_theoretical = classification_metrics(y, t)\n",
    "    scores.update(dict(\n",
    "        test_fbeta=result['fbeta'],\n",
    "        test_fbeta_cv=result_cv['fbeta'],\n",
    "        test_fbeta_theoretical=result_theoretical['fbeta'],\n",
    "        test_threshold_theoretical=result_theoretical['threshold'],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "51a956ee03328b1c2ab9dd589e9e0e4bad426d54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "submit:   0%|          | 0/367 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:179: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n",
      "                                                         \r"
     ]
    }
   ],
   "source": [
    "# Predict submit datasets\n",
    "submit_y = ensembler.predict(submit_dataset.X, submit_dataset.X2)\n",
    "submit_df['prediction'] = submit_y\n",
    "submit_df = submit_df[['qid', 'prediction']]\n",
    "submit_df.to_csv(config.outdir / 'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "d9843e601e8201f29a9394f794b83e9b7a9f0d5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ap</th>\n",
       "      <th>fbeta</th>\n",
       "      <th>loss</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>rocauc</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>0.596646</td>\n",
       "      <td>0.590578</td>\n",
       "      <td>0.128037</td>\n",
       "      <td>0.607087</td>\n",
       "      <td>0.574943</td>\n",
       "      <td>0.943649</td>\n",
       "      <td>0.458763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.709571</td>\n",
       "      <td>0.680630</td>\n",
       "      <td>0.097731</td>\n",
       "      <td>0.630397</td>\n",
       "      <td>0.739561</td>\n",
       "      <td>0.967645</td>\n",
       "      <td>0.317466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.736355</td>\n",
       "      <td>0.699558</td>\n",
       "      <td>0.091723</td>\n",
       "      <td>0.653197</td>\n",
       "      <td>0.753002</td>\n",
       "      <td>0.972544</td>\n",
       "      <td>0.333545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.758542</td>\n",
       "      <td>0.718836</td>\n",
       "      <td>0.086301</td>\n",
       "      <td>0.680458</td>\n",
       "      <td>0.761801</td>\n",
       "      <td>0.976387</td>\n",
       "      <td>0.357082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.780958</td>\n",
       "      <td>0.735228</td>\n",
       "      <td>0.081083</td>\n",
       "      <td>0.685713</td>\n",
       "      <td>0.792450</td>\n",
       "      <td>0.979804</td>\n",
       "      <td>0.341354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <th>4</th>\n",
       "      <td>0.724046</td>\n",
       "      <td>0.694131</td>\n",
       "      <td>0.100259</td>\n",
       "      <td>0.667485</td>\n",
       "      <td>0.722992</td>\n",
       "      <td>0.968281</td>\n",
       "      <td>0.280633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ap     fbeta    ...        rocauc  threshold\n",
       "name  epoch                        ...                         \n",
       "train 0      0.596646  0.590578    ...      0.943649   0.458763\n",
       "      1      0.709571  0.680630    ...      0.967645   0.317466\n",
       "      2      0.736355  0.699558    ...      0.972544   0.333545\n",
       "      3      0.758542  0.718836    ...      0.976387   0.357082\n",
       "      4      0.780958  0.735228    ...      0.979804   0.341354\n",
       "valid 4      0.724046  0.694131    ...      0.968281   0.280633\n",
       "\n",
       "[6 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ap</th>\n",
       "      <th>fbeta</th>\n",
       "      <th>loss</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>rocauc</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>0.601152</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.126777</td>\n",
       "      <td>0.599867</td>\n",
       "      <td>0.588329</td>\n",
       "      <td>0.944340</td>\n",
       "      <td>0.447920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.708911</td>\n",
       "      <td>0.681134</td>\n",
       "      <td>0.097805</td>\n",
       "      <td>0.632266</td>\n",
       "      <td>0.738190</td>\n",
       "      <td>0.967712</td>\n",
       "      <td>0.322529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.737729</td>\n",
       "      <td>0.701711</td>\n",
       "      <td>0.091431</td>\n",
       "      <td>0.661960</td>\n",
       "      <td>0.746541</td>\n",
       "      <td>0.972680</td>\n",
       "      <td>0.351150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.762495</td>\n",
       "      <td>0.719842</td>\n",
       "      <td>0.085700</td>\n",
       "      <td>0.668733</td>\n",
       "      <td>0.779409</td>\n",
       "      <td>0.976751</td>\n",
       "      <td>0.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.783384</td>\n",
       "      <td>0.735415</td>\n",
       "      <td>0.080696</td>\n",
       "      <td>0.691561</td>\n",
       "      <td>0.785207</td>\n",
       "      <td>0.980002</td>\n",
       "      <td>0.358615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <th>4</th>\n",
       "      <td>0.727337</td>\n",
       "      <td>0.695116</td>\n",
       "      <td>0.096494</td>\n",
       "      <td>0.658745</td>\n",
       "      <td>0.735738</td>\n",
       "      <td>0.969432</td>\n",
       "      <td>0.341036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ap     fbeta    ...        rocauc  threshold\n",
       "name  epoch                        ...                         \n",
       "train 0      0.601152  0.594042    ...      0.944340   0.447920\n",
       "      1      0.708911  0.681134    ...      0.967712   0.322529\n",
       "      2      0.737729  0.701711    ...      0.972680   0.351150\n",
       "      3      0.762495  0.719842    ...      0.976751   0.335600\n",
       "      4      0.783384  0.735415    ...      0.980002   0.358615\n",
       "valid 4      0.727337  0.695116    ...      0.969432   0.341036\n",
       "\n",
       "[6 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ap</th>\n",
       "      <th>fbeta</th>\n",
       "      <th>loss</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>rocauc</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>0.600343</td>\n",
       "      <td>0.592925</td>\n",
       "      <td>0.127272</td>\n",
       "      <td>0.605421</td>\n",
       "      <td>0.580935</td>\n",
       "      <td>0.944038</td>\n",
       "      <td>0.455227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.706163</td>\n",
       "      <td>0.681532</td>\n",
       "      <td>0.097823</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.732372</td>\n",
       "      <td>0.967795</td>\n",
       "      <td>0.333222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.733551</td>\n",
       "      <td>0.701342</td>\n",
       "      <td>0.091701</td>\n",
       "      <td>0.658263</td>\n",
       "      <td>0.750453</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.344719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.757809</td>\n",
       "      <td>0.720015</td>\n",
       "      <td>0.086083</td>\n",
       "      <td>0.685576</td>\n",
       "      <td>0.758098</td>\n",
       "      <td>0.976591</td>\n",
       "      <td>0.368143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.780415</td>\n",
       "      <td>0.737826</td>\n",
       "      <td>0.080679</td>\n",
       "      <td>0.693914</td>\n",
       "      <td>0.787670</td>\n",
       "      <td>0.980091</td>\n",
       "      <td>0.357861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <th>4</th>\n",
       "      <td>0.728680</td>\n",
       "      <td>0.692526</td>\n",
       "      <td>0.096815</td>\n",
       "      <td>0.645227</td>\n",
       "      <td>0.747309</td>\n",
       "      <td>0.970399</td>\n",
       "      <td>0.366123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ap     fbeta    ...        rocauc  threshold\n",
       "name  epoch                        ...                         \n",
       "train 0      0.600343  0.592925    ...      0.944038   0.455227\n",
       "      1      0.706163  0.681532    ...      0.967795   0.333222\n",
       "      2      0.733551  0.701342    ...      0.972603   0.344719\n",
       "      3      0.757809  0.720015    ...      0.976591   0.368143\n",
       "      4      0.780415  0.737826    ...      0.980091   0.357861\n",
       "valid 4      0.728680  0.692526    ...      0.970399   0.366123\n",
       "\n",
       "[6 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ap</th>\n",
       "      <th>fbeta</th>\n",
       "      <th>loss</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>rocauc</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>0.598902</td>\n",
       "      <td>0.595143</td>\n",
       "      <td>0.127592</td>\n",
       "      <td>0.615205</td>\n",
       "      <td>0.576347</td>\n",
       "      <td>0.944087</td>\n",
       "      <td>0.460825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.708959</td>\n",
       "      <td>0.680674</td>\n",
       "      <td>0.097863</td>\n",
       "      <td>0.642340</td>\n",
       "      <td>0.723873</td>\n",
       "      <td>0.967747</td>\n",
       "      <td>0.340972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.736360</td>\n",
       "      <td>0.702154</td>\n",
       "      <td>0.091569</td>\n",
       "      <td>0.663700</td>\n",
       "      <td>0.745339</td>\n",
       "      <td>0.972729</td>\n",
       "      <td>0.347786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.761054</td>\n",
       "      <td>0.719428</td>\n",
       "      <td>0.085872</td>\n",
       "      <td>0.679335</td>\n",
       "      <td>0.764550</td>\n",
       "      <td>0.976802</td>\n",
       "      <td>0.356295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.783657</td>\n",
       "      <td>0.735433</td>\n",
       "      <td>0.080413</td>\n",
       "      <td>0.701362</td>\n",
       "      <td>0.772984</td>\n",
       "      <td>0.980325</td>\n",
       "      <td>0.375891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <th>4</th>\n",
       "      <td>0.726105</td>\n",
       "      <td>0.694662</td>\n",
       "      <td>0.098082</td>\n",
       "      <td>0.654940</td>\n",
       "      <td>0.739512</td>\n",
       "      <td>0.969891</td>\n",
       "      <td>0.335010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ap     fbeta    ...        rocauc  threshold\n",
       "name  epoch                        ...                         \n",
       "train 0      0.598902  0.595143    ...      0.944087   0.460825\n",
       "      1      0.708959  0.680674    ...      0.967747   0.340972\n",
       "      2      0.736360  0.702154    ...      0.972729   0.347786\n",
       "      3      0.761054  0.719428    ...      0.976802   0.356295\n",
       "      4      0.783657  0.735433    ...      0.980325   0.375891\n",
       "valid 4      0.726105  0.694662    ...      0.969891   0.335010\n",
       "\n",
       "[6 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ap</th>\n",
       "      <th>fbeta</th>\n",
       "      <th>loss</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>rocauc</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train</th>\n",
       "      <th>0</th>\n",
       "      <td>0.601793</td>\n",
       "      <td>0.598171</td>\n",
       "      <td>0.126842</td>\n",
       "      <td>0.595742</td>\n",
       "      <td>0.600619</td>\n",
       "      <td>0.944147</td>\n",
       "      <td>0.430279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.709774</td>\n",
       "      <td>0.680551</td>\n",
       "      <td>0.097755</td>\n",
       "      <td>0.638679</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.967731</td>\n",
       "      <td>0.334086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.737244</td>\n",
       "      <td>0.701700</td>\n",
       "      <td>0.091395</td>\n",
       "      <td>0.664953</td>\n",
       "      <td>0.742746</td>\n",
       "      <td>0.972781</td>\n",
       "      <td>0.353970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.761399</td>\n",
       "      <td>0.720250</td>\n",
       "      <td>0.085702</td>\n",
       "      <td>0.678001</td>\n",
       "      <td>0.768115</td>\n",
       "      <td>0.976782</td>\n",
       "      <td>0.352841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.785705</td>\n",
       "      <td>0.737458</td>\n",
       "      <td>0.080156</td>\n",
       "      <td>0.691142</td>\n",
       "      <td>0.790428</td>\n",
       "      <td>0.980340</td>\n",
       "      <td>0.348976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <th>4</th>\n",
       "      <td>0.724224</td>\n",
       "      <td>0.692502</td>\n",
       "      <td>0.096250</td>\n",
       "      <td>0.644678</td>\n",
       "      <td>0.747989</td>\n",
       "      <td>0.970443</td>\n",
       "      <td>0.299205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ap     fbeta    ...        rocauc  threshold\n",
       "name  epoch                        ...                         \n",
       "train 0      0.601793  0.598171    ...      0.944147   0.430279\n",
       "      1      0.709774  0.680551    ...      0.967731   0.334086\n",
       "      2      0.737244  0.701700    ...      0.972781   0.353970\n",
       "      3      0.761399  0.720250    ...      0.976782   0.352841\n",
       "      4      0.785705  0.737458    ...      0.980340   0.348976\n",
       "valid 4      0.724224  0.692502    ...      0.970443   0.299205\n",
       "\n",
       "[6 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (train, valid) in enumerate(zip(train_results, valid_results)):\n",
    "    df = pd.concat([train.summary, valid.summary])\n",
    "    df = df.set_index(['name', df.index], drop=True)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "9cffa7ca15338c9add8cf2ba5a2aa0f5891641ba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>threshold</th>\n",
       "      <th>threshold_cv</th>\n",
       "      <th>valid_epoch</th>\n",
       "      <th>valid_fbeta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7004.711669</td>\n",
       "      <td>0.324402</td>\n",
       "      <td>0.324402</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.693787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   elapsed_time  threshold  threshold_cv  valid_epoch  valid_fbeta\n",
       "0   7004.711669   0.324402      0.324402          4.0     0.693787"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "f9cda12442afd1869d05c8a0d3beb00a64b94ddf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
